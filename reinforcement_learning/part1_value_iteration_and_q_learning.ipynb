{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "private_outputs": true,
      "authorship_tag": "ABX9TyOH0gX54auuHpnBIRO9YXh8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eemlcommunity/PracticalSessions2023/blob/omardd%2Frl/part1_value_iteration_and_q_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# [EEML 2023] Reinforcement Learning Tutorial - Part 1\n",
        "\n",
        "## Value Iteration & Q-Learning\n"
      ],
      "metadata": {
        "id": "RxNFbYjlD8jb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Colab Setup"
      ],
      "metadata": {
        "id": "SLED9VRZE_B3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmNNXZZTDfUQ"
      },
      "outputs": [],
      "source": [
        "# Colab setup\n",
        "from IPython import get_ipython\n",
        "\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "  # install rlberry library (https://github.com/rlberry-py/rlberry)\n",
        "  !pip install rlberry==0.5.0 > /dev/null 2>&1\n",
        "\n",
        "  # install ffmpeg-python for saving videos\n",
        "  !pip install ffmpeg-python > /dev/null 2>&1\n",
        "\n",
        "  # packages required to show video\n",
        "  !pip install pyvirtualdisplay > /dev/null 2>&1\n",
        "  !apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "\n",
        "# Check rlberry version\n",
        "import rlberry\n",
        "print(rlberry.__version__)\n",
        "\n",
        "# Create directory for saving videos\n",
        "!mkdir videos > /dev/null 2>&1\n",
        "\n",
        "# Initialize display and import function to show videos\n",
        "import rlberry.colab_utils.display_setup\n",
        "from rlberry.colab_utils.display_setup import show_video"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "from rlberry.envs import GridWorld"
      ],
      "metadata": {
        "id": "ZqyFwE9ME8Op"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_env(easy=False):\n",
        "  \"\"\"Creates an instance of a grid-world MDP.\"\"\"\n",
        "  if easy:\n",
        "    env = GridWorld(\n",
        "        nrows=3,\n",
        "        ncols=3,\n",
        "        walls=(),\n",
        "        reward_at = {(2, 2): 1.0},\n",
        "        success_probability=0.9,\n",
        "        terminal_states = ((2, 2),),\n",
        "    )\n",
        "  else:\n",
        "    env = GridWorld(\n",
        "        nrows=5,\n",
        "        ncols=7,\n",
        "        reward_at = {(0, 6): 1.0},\n",
        "        walls=((0, 4), (1, 4), (2, 4), (3, 4)),\n",
        "        success_probability=0.9,\n",
        "        terminal_states=((0, 6),)\n",
        "    )\n",
        "  return env\n",
        "\n",
        "def render_policy(env, policy=None, horizon=50):\n",
        "  \"\"\"Visualize a policy in an environment\n",
        "\n",
        "  Args:\n",
        "    env: GridWorld\n",
        "        environment where to run the policy\n",
        "    policy: np.array\n",
        "        matrix mapping states to action (Ns).\n",
        "        If None, runs random policy.\n",
        "    horizon: int\n",
        "        maximum number of timesteps in the environment.\n",
        "  \"\"\"\n",
        "  env.enable_rendering()\n",
        "  state, info = env.reset()                       # get initial state\n",
        "  for timestep in range(horizon):\n",
        "      if policy is None:\n",
        "        action = env.action_space.sample()  # take random actions\n",
        "      else:\n",
        "        action = policy[state]\n",
        "      next_state, reward, terminated, truncated, info = env.step(action)\n",
        "      state = next_state\n",
        "      if terminated or truncated:\n",
        "        break\n",
        "  # save video and clear buffer\n",
        "  env.save_video('./videos/gw.mp4', framerate=5)\n",
        "  env.clear_render_buffer()\n",
        "  env.disable_rendering()\n",
        "  # show video\n",
        "  show_video('./videos/gw.mp4')"
      ],
      "metadata": {
        "id": "UckbeW7zFEki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an environment and visualize it\n",
        "env = get_env()\n",
        "render_policy(env)  # visualize random policy\n",
        "\n",
        "# The reward function and transition probabilities can be accessed through\n",
        "# the R and P attributes:\n",
        "print(f\"Shape of the reward array = (S, A) = {env.R.shape}\")\n",
        "print(f\"Shape of the transition array = (S, A, S) = {env.P.shape}\")\n",
        "print(f\"Reward at (s, a) = (1, 0): {env.R[1, 0]}\")\n",
        "print(f\"Prob[s\\'=2 | s=1, a=0]: {env.P[1, 0, 2]}\")\n",
        "print(f\"Number of states and actions: {env.Ns}, {env.Na}\")\n",
        "\n",
        "# The states in the griworld correspond to (row, col) coordinates.\n",
        "# The environment provides a mapping between (row, col) and the index of\n",
        "# each state:\n",
        "print(f\"Index of state (1, 0): {env.coord2index[(1, 0)]}\")\n",
        "print(f\"Coordinates of state 5: {env.index2coord[5]}\")"
      ],
      "metadata": {
        "id": "xfFMORb3FIXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Value Iteration\n",
        "\n",
        "\n",
        "Complete the function value_iteration below.\n",
        "\n",
        "### What to implement\n",
        "\n",
        "The application of the Bellman operator:\n",
        "\n",
        "$$\n",
        "T^* Q(s, a) =  R(s, a) + \\gamma \\sum_{s'} P(s'|s,a) \\max_{a'} Q(s', a')\n",
        "$$"
      ],
      "metadata": {
        "id": "neUn7m1TFPKI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def value_iteration(P, R, gamma=0.9, tol=1e-3):\n",
        "    \"\"\"\n",
        "    Run value iteration\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    P: np.array\n",
        "        transition matrix (Ns, Na, Ns)\n",
        "    R: np.array\n",
        "        reward matrix (Ns, Na)\n",
        "    gamma: float\n",
        "        discount factor\n",
        "    tol: float\n",
        "        precision of the solution\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Q, greedy_policy, Qfs\n",
        "      Q: final Q-function (at iteration n)\n",
        "      greedy_policy: greedy policy wrt Qn\n",
        "      Qfs: all Q-functions generated by the algorithm (for visualization)\n",
        "    \"\"\"\n",
        "    Ns, Na = R.shape\n",
        "    Q = np.zeros((Ns, Na))\n",
        "    TQ = np.zeros((Ns, Na))\n",
        "    Qfs = [Q]\n",
        "    err = np.inf\n",
        "    while err > tol:\n",
        "      # ====================================================\n",
        "      # YOUR IMPLEMENTATION HERE\n",
        "      # compute TQ ...\n",
        "      # ====================================================\n",
        "      err = np.abs(TQ - Q).max()\n",
        "      Q = TQ\n",
        "      Qfs.append(Q)\n",
        "\n",
        "    greedy_policy = np.argmax(Q, axis=1)\n",
        "    return Q, greedy_policy, Qfs"
      ],
      "metadata": {
        "id": "9D8duzPsFgYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# Running Value Iteration\n",
        "#\n",
        "\n",
        "# Parameters\n",
        "tol = 1e-5\n",
        "gamma = 0.99\n",
        "\n",
        "# Environment\n",
        "env = get_env()\n",
        "\n",
        "# run value iteration to obtain Q-values\n",
        "VI_Q, VI_greedypol, all_qfunctions = value_iteration(\n",
        "    env.P, env.R, gamma=gamma, tol=tol)\n",
        "\n",
        "# render the policy\n",
        "print(\"Greedy policy obtained from value iteration\")\n",
        "render_policy(env, VI_greedypol)\n",
        "\n",
        "# show the error between the computed V-functions and the final V-function\n",
        "# (that should be the approximatele the optimal one, if correctly implemented)\n",
        "# as a function of the number of iterations\n",
        "final_V = all_qfunctions[-1].max(axis=1)\n",
        "norms = [ np.abs(q.max(axis=1) - final_V).max() for q in all_qfunctions]\n",
        "plt.plot(norms)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Error')\n",
        "plt.title(\"Value Iteration Convergence\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pvBEZWe-FjO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-Learning\n",
        "\n",
        "### What to implement\n",
        "Finish the implementation of the function ``q_learning`` that takes as input an environment, runs Q learning for $T$ time steps and returns $Q_T$.\n",
        "\n",
        "**Implement the update to be applied to the Q function**:\n",
        "\n",
        "$$\\delta_t = r_t + \\gamma \\max_a Q_t(s_{t+1}, a) - Q_t(s_t, a_t)$$\n",
        "\n",
        "### What you can play with\n",
        "You can test different learning rates:\n",
        "  * $\\alpha_t(s, a) =$ constant in $]0, 1[$\n",
        "  * $\\alpha_t(s, a) = \\frac{1}{\\text{number of visits to} (s, a)}$\n",
        "  * others?\n",
        "\n",
        "You can also test different initializations of the Q function and try different values of $\\varepsilon$ in the $\\varepsilon$-greedy exploration.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qBwr7ndBFvTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def q_learning(env, n_iterations, gamma, store_q_interval):\n",
        "    \"\"\"\n",
        "    Implementation of Q-Learning\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    env: gym.Env\n",
        "        environment\n",
        "    n_iterations: int\n",
        "        number of Q-learning iterations\n",
        "    gamma: float\n",
        "      discount factor\n",
        "    store_q_interval: int\n",
        "      interval (in number of iterations) to store q functions\n",
        "\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Q, greedy_policy, Qfs, N_visits\n",
        "      Q: final Q-function (at iteration n)\n",
        "      greedy_policy: greedy policy wrt Qn\n",
        "      Qfs: all Q-functions generated by the algorithm (for visualization)\n",
        "      N_visits: number of visits to each state-action pair\n",
        "    \"\"\"\n",
        "    Ns, Na = env.R.shape\n",
        "    Q = np.zeros((Ns, Na))  # can we improve this initialization?\n",
        "    N = np.zeros((Ns, Na))  # number of visits to each (s, a)\n",
        "\n",
        "    Qfs = [Q.copy()]\n",
        "\n",
        "\n",
        "    # epsilon for exploration (you can change it, make it depend on time, etc.)\n",
        "    epsilon = 0.5\n",
        "\n",
        "    state, info = env.reset()\n",
        "    for tt in tqdm(range(n_iterations), desc=\"running q_learning\"):\n",
        "      # epsilon-greedy exploration\n",
        "      if np.random.uniform() < epsilon:  # happers with prob epsilon\n",
        "        action = env.action_space.sample()\n",
        "      else:\n",
        "        action = Q[state, :].argmax()\n",
        "\n",
        "      next_state, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "      # ====================================================\n",
        "\t    # YOUR IMPLEMENTATION HERE\n",
        "      #\n",
        "      # take action, observe next state and reward\n",
        "\n",
        "      # compute delta_t\n",
        "      delta = 0 # ...\n",
        "      # ====================================================\n",
        "\n",
        "      # update Q\n",
        "      alpha = 0.1\n",
        "      Q[state, action] += alpha*delta\n",
        "\n",
        "      # update number of visits\n",
        "      N[state, action] += 1\n",
        "\n",
        "      # update state\n",
        "      if terminated or truncated:\n",
        "        state, _ = env.reset()\n",
        "      else:\n",
        "        state = next_state\n",
        "\n",
        "\n",
        "      # store Q function\n",
        "      if (tt % store_q_interval) == 0:\n",
        "        Qfs.append(Q.copy())\n",
        "    greedy_policy = np.argmax(Q, axis=1)\n",
        "    return Q, greedy_policy, Qfs, N"
      ],
      "metadata": {
        "id": "kUi0iIEcFw6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# Running Q-Learning\n",
        "#\n",
        "\n",
        "# Parameters\n",
        "store_q_interval = 1000   #@param {type:\"integer\"}\n",
        "n_iterations = 2500000  #@param {type:\"integer\"}\n",
        "gamma = 0.99  #@param {type:\"number\"}\n",
        "easy_enviroment = False  #@param {type:\"boolean\"}\n",
        "\n",
        "# Environment\n",
        "# Start with easy=True, then try easy=False\n",
        "env = get_env(easy=easy_enviroment)\n",
        "\n",
        "# Get ground truth from Value Iteration\n",
        "VI_Q, _, _ = value_iteration(env.P, env.R, gamma=gamma, tol=tol)\n",
        "VI_V = VI_Q.max(axis=1)\n",
        "\n",
        "# run value iteration to obtain Q-values\n",
        "QL_Q, QL_greedypol, all_qfunctions_ql, N_visits =  q_learning(\n",
        "    env, n_iterations, gamma, store_q_interval)\n",
        "\n",
        "# render the policy\n",
        "print(\"Greedy policy obtained with Q Learning\")\n",
        "render_policy(env, QL_greedypol)\n",
        "\n",
        "# show the error between the V functions obtained in Q Learning, and the\n",
        "# final V function obtained with Value Iteration\n",
        "norms = [ np.abs(q.max(axis=1) - VI_V).max() for q in all_qfunctions_ql]\n",
        "n_iterations = (1 + np.arange(len(norms))) * store_q_interval\n",
        "plt.plot(n_iterations, norms)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Error')\n",
        "plt.title(\"Q-Learning Convergence\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5KPEWl7LF6Dv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}