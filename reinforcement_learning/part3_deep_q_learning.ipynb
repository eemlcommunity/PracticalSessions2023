{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "private_outputs": true,
      "authorship_tag": "ABX9TyOQangmGJFiK73hnvyiRa2h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eemlcommunity/PracticalSessions2023/blob/omardd%2Frl/reinforcement_learning/part3_deep_q_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [EEML 2023] Reinforcement Learning Tutorial - Part 3\n",
        "\n",
        "## Deep Q-Learning"
      ],
      "metadata": {
        "id": "hbOJ5sLIEGXH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Colab Setup"
      ],
      "metadata": {
        "id": "vfngPqCoIKCI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BTLIio04DuXm"
      },
      "outputs": [],
      "source": [
        "# Colab setup\n",
        "from IPython import get_ipython\n",
        "\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "  # optax, haiku, rlax\n",
        "  !pip install optax > /dev/null 2>&1\n",
        "  !pip install dm-haiku > /dev/null 2>&1\n",
        "  !pip install rlax > /dev/null 2>&1\n",
        "\n",
        "  # gymnasium\n",
        "  !pip install -q swig > /dev/null 2>&1\n",
        "  !pip install \"gymnasium[box2d]\" > /dev/null 2>&1\n",
        "\n",
        "  # install rlberry library (https://github.com/rlberry-py/rlberry)\n",
        "  !pip install rlberry==0.5.0 > /dev/null 2>&1\n",
        "\n",
        "  # reinstall numpy to avoid errors\n",
        "  !pip install \"numpy<1.23.0\" > /dev/null 2>&1\n",
        "\n",
        "  # install ffmpeg-python for saving videos\n",
        "  !pip install ffmpeg-python > /dev/null 2>&1\n",
        "\n",
        "  # packages required to show video\n",
        "  !pip install pyvirtualdisplay > /dev/null 2>&1\n",
        "  !apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "\n",
        "# Check rlberry version\n",
        "import rlberry\n",
        "print(rlberry.__version__)\n",
        "\n",
        "# Create directory for saving videos\n",
        "!mkdir videos > /dev/null 2>&1\n",
        "\n",
        "# Initialize display and import function to show videos\n",
        "import rlberry.colab_utils.display_setup\n",
        "from rlberry.colab_utils.display_setup import show_video"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import rlberry\n",
        "print(np.__version__)"
      ],
      "metadata": {
        "id": "Z6x4DtjAapC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Useful imports\n",
        "import gymnasium as gym\n",
        "from gymnasium.utils.save_video import save_video\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from copy import deepcopy\n",
        "from rlberry.agents import Agent\n",
        "from rlberry.manager import AgentManager, plot_writer_data, read_writer_data\n",
        "from typing import Callable, NamedTuple, Sequence\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import chex\n",
        "import haiku as hk\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import optax\n",
        "\n",
        "# torch device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "b8-agpLqIgWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env_id = 'MountainCar-v0' #@param [\"CartPole-v1\", \"LunarLander-v2\", \"MountainCar-v0\"]"
      ],
      "metadata": {
        "id": "O6iH_cvrLtm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dqn_env(for_render=False):\n",
        "  if not for_render:\n",
        "    return gym.make(env_id)\n",
        "  else:\n",
        "    return gym.make(env_id, render_mode=\"rgb_array_list\")\n",
        "\n",
        "def render_dqn_policy(agent=None):\n",
        "  env = get_dqn_env(for_render=True)\n",
        "  state, _ = env.reset()\n",
        "  step_starting_index = 0\n",
        "  episode_index = 0\n",
        "  for step_index in range(500):\n",
        "    if episode_index > 0: # show only one episode\n",
        "      break\n",
        "    if agent is None:\n",
        "      action = env.action_space.sample()\n",
        "    else:\n",
        "      action = agent.policy(state)\n",
        "    next_state, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "    if terminated or truncated:\n",
        "        save_video(\n",
        "          env.render(),\n",
        "          \"videos\",\n",
        "          fps=env.metadata[\"render_fps\"],\n",
        "          step_starting_index=step_starting_index,\n",
        "          episode_index=episode_index\n",
        "        )\n",
        "        step_starting_index = step_index + 1\n",
        "        episode_index += 1\n",
        "        next_state, _ = env.reset()\n",
        "    state = next_state\n",
        "  show_video(\"videos/rl-video-episode-0.mp4\")\n",
        "\n",
        "\n",
        "# render_dqn_policy()"
      ],
      "metadata": {
        "id": "6bMTdcSbIzAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Replay Buffer"
      ],
      "metadata": {
        "id": "tHNzw_TvJk9B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "  def __init__(self, capacity, rng):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    capacity : int\n",
        "      Maximum number of transitions\n",
        "    rng :\n",
        "      instance of numpy's default_rng\n",
        "    \"\"\"\n",
        "    self.capacity = capacity\n",
        "    self.rng = rng  # random number generator\n",
        "    self.memory = []\n",
        "    self.position = 0\n",
        "\n",
        "  def push(self, sample):\n",
        "    \"\"\"Saves a transition.\"\"\"\n",
        "    if len(self.memory) < self.capacity:\n",
        "      self.memory.append(None)\n",
        "    self.memory[self.position] = sample\n",
        "    self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    indices = self.rng.choice(len(self.memory), size=batch_size)\n",
        "    samples = [self.memory[idx] for idx in indices]\n",
        "    return map(np.asarray, zip(*samples))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.memory)"
      ],
      "metadata": {
        "id": "oBiPXk-iJmsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network Architecture"
      ],
      "metadata": {
        "id": "DBiF3Y34JqZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import haiku as hk\n",
        "from typing import Tuple\n",
        "\n",
        "\n",
        "class MLPQNetwork(hk.Module):\n",
        "    \"\"\"\n",
        "    MLP for Q functions with discrete number of actions.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    num_actions : int\n",
        "        Number of actions.\n",
        "    hidden_sizes : Tuple[int, ...]\n",
        "        Number of hidden layers in the MLP.\n",
        "    name : str\n",
        "        Identifier of the module.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, num_actions: int, hidden_sizes: Tuple[int, ...] = (64, 64), name: str = \"MLPQNetwork\"\n",
        "    ):\n",
        "        super().__init__(name=name)\n",
        "        self._mlp = hk.nets.MLP(output_sizes=hidden_sizes + (num_actions,))\n",
        "\n",
        "    def __call__(self, observation):\n",
        "        out = self._mlp(observation)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "DoFmSEbuJpww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameters\n"
      ],
      "metadata": {
        "id": "5gm99QYhJ1Cz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "\n",
        "# Number of timesteps for training.\n",
        "DQN_TRAINING_TIMESTEPS = 10000  #@param {type:\"integer\"}\n",
        "# Discount factor\n",
        "GAMMA = 0.99  #@param {type:\"number\"}\n",
        "# Batch size (in number of chunks).\n",
        "BATCH_SIZE = 64  #@param {type:\"integer\"}\n",
        "# Size of trajectory chunks to sample from the buffer.\n",
        "CHUNK_SIZE = 8  #@param {type:\"integer\"}\n",
        "# Interval (in number of transitions) between updates of the online network.\n",
        "ONLINE_UPDATE_INTERVAL = 1  #@param {type:\"integer\"}\n",
        "# Interval (in number total number of online updates) between updates of the target network.\n",
        "TARGET_UPDATE_INTERVAL = 512  #@param {type:\"integer\"}\n",
        "# Learning rate\n",
        "LEARNING_RATE = 0.001 #@param {type:\"number\"}\n",
        "# Initial value of epsilon\n",
        "EPSILON_INIT = 1.0  #@param {type:\"number\"}\n",
        "# Minimum value of epsilon\n",
        "EPSILON_END = 0.05  #@param {type:\"number\"}\n",
        "# Parameter to decrease epsilon\n",
        "EPSILON_STEPS = 5000  #@param {type:\"integer\"}\n",
        "# Maximum size of replay buffer\n",
        "MAX_REPLAY_SIZE = 100000  #@param {type:\"integer\"}\n",
        "# Interval (in number of transitions) between agent evaluations in fit().\n",
        "EVAL_INTERVAL = 256 #@param {type:\"integer\"}\n",
        "\n",
        "\n",
        "DQN_PARAMS = dict(\n",
        "    gamma=GAMMA,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    chunk_size=CHUNK_SIZE,\n",
        "    online_update_interval=ONLINE_UPDATE_INTERVAL,\n",
        "    target_update_interval=TARGET_UPDATE_INTERVAL,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    epsilon_init=EPSILON_INIT,\n",
        "    epsilon_end=EPSILON_END,\n",
        "    epsilon_steps=EPSILON_STEPS,\n",
        "    max_replay_size=MAX_REPLAY_SIZE,\n",
        "    eval_interval=EVAL_INTERVAL,\n",
        ")"
      ],
      "metadata": {
        "id": "LAOakJ96J3Bl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DQN Agent Implementation\n",
        "\n",
        "\n",
        "Implement the DQN loss in the `_loss` method of the `DQNAgent` class below.\n",
        "\n",
        "The loss is given by:\n",
        "\n",
        "\n",
        "$$\n",
        "L(\\theta) = \\sum_{(s_i, a_i, r_i, s_i') \\in \\mathcal{B}}\n",
        "\\left[\n",
        "Q(s_i, a_i, \\theta) -  y_i\n",
        "\\right]^2\n",
        "$$\n",
        "where the $y_i$ are the **targets** computed with the **target network** $\\theta^-$:\n",
        "\n",
        "\n",
        "$$\n",
        "y_i = r_i + \\gamma \\max_{a'} Q(s_i', a', \\theta^-).\n",
        "$$\n",
        "\n",
        "Note: we call **online network** the one parameterized by $\\theta$, since it's the one used to interact with the environment (online)."
      ],
      "metadata": {
        "id": "wjuBUp2sJ_PZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import chex\n",
        "import functools\n",
        "import haiku as hk\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "import numpy as np\n",
        "import optax\n",
        "import dill\n",
        "import rlax\n",
        "\n",
        "from gymnasium import spaces\n",
        "from pathlib import Path\n",
        "from rlberry import types\n",
        "from rlberry.agents import AgentWithSimplePolicy\n",
        "from rlberry.agents.utils.replay import ReplayBuffer\n",
        "from typing import Any, Callable, Mapping, Optional\n",
        "\n",
        "import rlberry\n",
        "\n",
        "logger = rlberry.logger\n",
        "\n",
        "\n",
        "@chex.dataclass\n",
        "class AllParams:\n",
        "    online: chex.ArrayTree\n",
        "    target: chex.ArrayTree\n",
        "\n",
        "\n",
        "@chex.dataclass\n",
        "class AllStates:\n",
        "    optimizer: chex.ArrayTree\n",
        "    learner_steps: int\n",
        "    actor_steps: int\n",
        "\n",
        "\n",
        "@chex.dataclass\n",
        "class ActorOutput:\n",
        "    actions: chex.Array\n",
        "    q_values: chex.Array\n",
        "\n",
        "\n",
        "class DQNAgent(AgentWithSimplePolicy):\n",
        "    \"\"\"\n",
        "    Implementation of Deep Q-Learning using JAX.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    env : types.Env\n",
        "        Environment.\n",
        "    gamma : float\n",
        "        Discount factor.\n",
        "    batch_size : int\n",
        "        Batch size (in number of chunks).\n",
        "    chunk_size : int\n",
        "        Size of trajectory chunks to sample from the buffer.\n",
        "    online_update_interval : int\n",
        "        Interval (in number of transitions) between updates of the online network.\n",
        "    target_update_interval : int\n",
        "        Interval (in number total number of online updates) between updates of the target network.\n",
        "    learning_rate : float\n",
        "        Optimizer learning rate.\n",
        "    epsilon_init : float\n",
        "        Initial value of epsilon-greedy exploration.\n",
        "    epsilon_end : float\n",
        "        End value of epsilon-greedy exploration.\n",
        "    epsilon_steps : int\n",
        "        Number of steps over which annealing over epsilon takes place.\n",
        "    max_replay_size : int\n",
        "        Maximum number of transitions in the replay buffer.\n",
        "    eval_interval : int\n",
        "        Interval (in number of transitions) between agent evaluations in fit().\n",
        "        If None, never evaluate.\n",
        "    max_episode_length : int\n",
        "        Maximum length of an episode. If None, episodes will only end if `done = True`\n",
        "        is returned by env.step().\n",
        "    net_constructor : callable\n",
        "        Constructor for Q network. If None, uses default MLP.\n",
        "    net_kwargs : dict\n",
        "        kwargs for network constructor (net_constructor).\n",
        "    max_gradient_norm : float, default: 100.0\n",
        "        Maximum gradient norm.\n",
        "    \"\"\"\n",
        "\n",
        "    name = \"JaxDqnAgent\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        env: types.Env,\n",
        "        gamma: float = 0.99,\n",
        "        batch_size: int = 64,\n",
        "        chunk_size: int = 8,\n",
        "        online_update_interval: int = 1,\n",
        "        target_update_interval: int = 512,\n",
        "        learning_rate: float = 0.001,\n",
        "        epsilon_init: float = 1.0,\n",
        "        epsilon_end: float = 0.05,\n",
        "        epsilon_steps: int = 5000,\n",
        "        max_replay_size: int = 100000,\n",
        "        eval_interval: Optional[int] = None,\n",
        "        max_episode_length: Optional[int] = None,\n",
        "        net_constructor: Optional[Callable[..., hk.Module]] = None,\n",
        "        net_kwargs: Optional[Mapping[str, Any]] = None,\n",
        "        max_gradient_norm: float = 100.0,\n",
        "        **kwargs\n",
        "    ):\n",
        "        AgentWithSimplePolicy.__init__(self, env, **kwargs)\n",
        "        env = self.env\n",
        "        self.rng_key = jax.random.PRNGKey(self.rng.integers(2**32).item())\n",
        "\n",
        "        # checks\n",
        "        if not isinstance(self.env.observation_space, spaces.Box):\n",
        "            raise ValueError(\"DQN only implemented for Box observation spaces.\")\n",
        "        if not isinstance(self.env.action_space, spaces.Discrete):\n",
        "            raise ValueError(\"DQN only implemented for Discrete action spaces.\")\n",
        "\n",
        "        # params\n",
        "        self._gamma = gamma\n",
        "        self._batch_size = batch_size\n",
        "        self._chunk_size = chunk_size\n",
        "        self._online_update_interval = online_update_interval\n",
        "        self._target_update_interval = target_update_interval\n",
        "        self._max_replay_size = max_replay_size\n",
        "        self._eval_interval = eval_interval\n",
        "        self._max_episode_length = max_episode_length or np.inf\n",
        "        self._max_gradient_norm = max_gradient_norm\n",
        "\n",
        "        #\n",
        "        # Setup replay buffer\n",
        "        #\n",
        "\n",
        "        # define specs\n",
        "        sample_obs, _ = env.reset()\n",
        "        try:\n",
        "            obs_shape, obs_dtype = sample_obs.shape, sample_obs.dtype\n",
        "        except AttributeError:  # in case sample_obs has no .shape attribute\n",
        "            obs_shape, obs_dtype = (\n",
        "                env.observation_space.shape,\n",
        "                env.observation_space.dtype,\n",
        "            )\n",
        "        action_shape, action_dtype = env.action_space.shape, env.action_space.dtype\n",
        "\n",
        "        # create replay buffer\n",
        "        self._replay_buffer = ReplayBuffer(\n",
        "            max_replay_size = self._max_replay_size,\n",
        "            rng=self.rng\n",
        "        )\n",
        "\n",
        "        self._replay_buffer.setup_entry(\"actions\", action_dtype)\n",
        "        self._replay_buffer.setup_entry(\"observations\", obs_dtype)\n",
        "        self._replay_buffer.setup_entry(\"next_observations\", obs_dtype)\n",
        "        self._replay_buffer.setup_entry(\"rewards\", np.float32)\n",
        "        self._replay_buffer.setup_entry(\"discounts\", np.float32)\n",
        "\n",
        "        # initialize network and params\n",
        "        net_constructor = net_constructor or MLPQNetwork\n",
        "        net_kwargs = net_kwargs or dict(\n",
        "            num_actions=self.env.action_space.n, hidden_sizes=(64, 64)\n",
        "        )\n",
        "        net_ctor = functools.partial(net_constructor, **net_kwargs)\n",
        "        self._q_net = hk.without_apply_rng(hk.transform(lambda x: net_ctor()(x)))\n",
        "\n",
        "        self._dummy_obs = jnp.ones(self.env.observation_space.shape)\n",
        "\n",
        "        self.rng_key, subkey1 = jax.random.split(self.rng_key)\n",
        "        self.rng_key, subkey2 = jax.random.split(self.rng_key)\n",
        "\n",
        "        self._all_params = AllParams(\n",
        "            online=self._q_net.init(subkey1, self._dummy_obs),\n",
        "            target=self._q_net.init(subkey2, self._dummy_obs),\n",
        "        )\n",
        "\n",
        "        # initialize optimizer and states\n",
        "        self._optimizer = optax.chain(\n",
        "            optax.clip_by_global_norm(self._max_gradient_norm),\n",
        "            optax.adam(learning_rate),\n",
        "        )\n",
        "        self._all_states = AllStates(\n",
        "            optimizer=self._optimizer.init(self._all_params.online),\n",
        "            learner_steps=jnp.array(0),\n",
        "            actor_steps=jnp.array(0),\n",
        "        )\n",
        "\n",
        "        # epsilon decay\n",
        "        self._epsilon_schedule = optax.polynomial_schedule(\n",
        "            init_value=epsilon_init,\n",
        "            end_value=epsilon_end,\n",
        "            transition_steps=epsilon_steps,\n",
        "            transition_begin=0,\n",
        "            power=1.0,\n",
        "        )\n",
        "\n",
        "        # update functions (jit)\n",
        "        self.actor_step = jax.jit(self._actor_step)\n",
        "        self.learner_step = jax.jit(self._learner_step)\n",
        "\n",
        "    def policy(self, observation):\n",
        "        self.rng_key, subkey = jax.random.split(self.rng_key)\n",
        "        actor_out, _ = self.actor_step(\n",
        "            self._all_params,\n",
        "            self._all_states,\n",
        "            observation,\n",
        "            subkey,\n",
        "            evaluation=True,\n",
        "        )\n",
        "        action = actor_out.actions.item()\n",
        "        return action\n",
        "\n",
        "    def fit(self, budget: int, **kwargs):\n",
        "        \"\"\"\n",
        "        Train DQN agent.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        budget: int\n",
        "            Number of timesteps to train the agent.\n",
        "        \"\"\"\n",
        "        del kwargs\n",
        "        timesteps_counter = 0\n",
        "        episode_rewards = 0.0\n",
        "        episode_timesteps = 0\n",
        "        observation, _ = self.env.reset()\n",
        "        while timesteps_counter < budget:\n",
        "            self.rng_key, subkey = jax.random.split(self.rng_key)\n",
        "            actor_out, self._all_states = self.actor_step(\n",
        "                self._all_params,\n",
        "                self._all_states,\n",
        "                observation,\n",
        "                subkey,\n",
        "                evaluation=False,\n",
        "            )\n",
        "            action = actor_out.actions.item()\n",
        "            next_obs, reward, terminated, truncated, info= self.env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            # check max episode length\n",
        "            done = done and (episode_timesteps < self._max_episode_length)\n",
        "\n",
        "            # store data\n",
        "            episode_rewards += reward\n",
        "            self._replay_buffer.append(\n",
        "                {\n",
        "                    \"actions\": action,\n",
        "                    \"observations\": observation,\n",
        "                    \"rewards\": reward,\n",
        "                    \"discounts\": self._gamma * (1.0 - done),\n",
        "                    \"next_observations\": next_obs,\n",
        "                }\n",
        "            )\n",
        "\n",
        "            # counters and next obs\n",
        "            timesteps_counter += 1\n",
        "            episode_timesteps += 1\n",
        "            observation = next_obs\n",
        "\n",
        "            # update\n",
        "            total_timesteps = self._all_states.actor_steps.item()\n",
        "            if total_timesteps % self._online_update_interval == 0:\n",
        "                if len(self._replay_buffer) > self._batch_size:\n",
        "                    sample = self._replay_buffer.sample(batch_size=self._batch_size, chunk_size=self._chunk_size)\n",
        "                    batch = sample.data\n",
        "                    self._all_params, self._all_states, info = self.learner_step(\n",
        "                        self._all_params, self._all_states, batch\n",
        "                    )\n",
        "                    if self.writer:\n",
        "                        self.writer.add_scalar(\n",
        "                            \"q_loss\", info[\"loss\"].item(), total_timesteps\n",
        "                        )\n",
        "                        self.writer.add_scalar(\n",
        "                            \"learner_steps\",\n",
        "                            self._all_states.learner_steps.item(),\n",
        "                            total_timesteps,\n",
        "                        )\n",
        "\n",
        "            # eval\n",
        "            if (\n",
        "                self._eval_interval is not None\n",
        "                and total_timesteps % self._eval_interval == 0\n",
        "            ):\n",
        "                eval_rewards = self.eval(\n",
        "                    eval_horizon=self._max_episode_length,\n",
        "                    n_simimulations=2,\n",
        "                    gamma=1.0,\n",
        "                )\n",
        "                self.writer.add_scalar(\n",
        "                    \"eval_rewards\", eval_rewards, total_timesteps\n",
        "                )\n",
        "\n",
        "            # check if episode ended\n",
        "            if done:\n",
        "                if self.writer:\n",
        "                    self.writer.add_scalar(\n",
        "                        \"episode_rewards\", episode_rewards, total_timesteps\n",
        "                    )\n",
        "                self._replay_buffer.end_episode()\n",
        "                episode_rewards = 0.0\n",
        "                episode_timesteps = 0\n",
        "                observation, _ = self.env.reset()\n",
        "\n",
        "    def _loss(self, all_params, batch):\n",
        "        obs_tm1 = batch[\"observations\"]\n",
        "        a_tm1 = batch[\"actions\"]\n",
        "        r_t = batch[\"rewards\"]\n",
        "        discount_t = batch[\"discounts\"]\n",
        "        obs_t = batch[\"next_observations\"]\n",
        "\n",
        "        # remove time (chunk) dim (batch has shape [batch, chunk_size, ...])\n",
        "        # they're reshaped to [batch * chunk_size, ...]\n",
        "        a_tm1 = a_tm1.flatten()\n",
        "        r_t = r_t.flatten()\n",
        "        discount_t = discount_t.flatten()\n",
        "        obs_tm1 = jnp.reshape(obs_tm1, (-1, obs_tm1.shape[-1]))\n",
        "        obs_t = jnp.reshape(obs_t, (-1, obs_t.shape[-1]))\n",
        "\n",
        "        q_tm1 = self._q_net.apply(all_params.online, obs_tm1)\n",
        "        q_t_val = self._q_net.apply(all_params.target, obs_t)\n",
        "        q_t_select = self._q_net.apply(all_params.online, obs_t)\n",
        "\n",
        "        # ====================================================\n",
        "        # YOUR IMPLEMENTATION HERE\n",
        "        #\n",
        "        loss = jnp.array(0.0) # ...\n",
        "        # ====================================================\n",
        "        info = dict(loss=loss)\n",
        "        return loss, info\n",
        "\n",
        "    def _actor_step(self, all_params, all_states, observation, rng_key, evaluation):\n",
        "        obs = jnp.expand_dims(observation, 0)  # dummy batch\n",
        "        q_val = self._q_net.apply(all_params.online, obs)[0]  # remove batch\n",
        "        epsilon = self._epsilon_schedule(all_states.actor_steps)\n",
        "        train_action = rlax.epsilon_greedy(epsilon).sample(rng_key, q_val)\n",
        "        eval_action = rlax.greedy().sample(rng_key, q_val)\n",
        "        action = jax.lax.select(evaluation, eval_action, train_action)\n",
        "        return (\n",
        "            ActorOutput(actions=action, q_values=q_val),\n",
        "            AllStates(\n",
        "                optimizer=all_states.optimizer,\n",
        "                learner_steps=all_states.learner_steps,\n",
        "                actor_steps=all_states.actor_steps + 1,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    def _learner_step(self, all_params, all_states, batch):\n",
        "        target_params = rlax.periodic_update(\n",
        "            all_params.online,\n",
        "            all_params.target,\n",
        "            all_states.learner_steps,\n",
        "            self._target_update_interval,\n",
        "        )\n",
        "        grad, info = jax.grad(self._loss, has_aux=True)(all_params, batch)\n",
        "        updates, optimizer_state = self._optimizer.update(\n",
        "            grad.online, all_states.optimizer\n",
        "        )\n",
        "        online_params = optax.apply_updates(all_params.online, updates)\n",
        "        return (\n",
        "            AllParams(online=online_params, target=target_params),\n",
        "            AllStates(\n",
        "                optimizer=optimizer_state,\n",
        "                learner_steps=all_states.learner_steps + 1,\n",
        "                actor_steps=all_states.actor_steps,\n",
        "            ),\n",
        "            info,\n",
        "        )"
      ],
      "metadata": {
        "id": "XRg0CyIus0zB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training & Evaluation"
      ],
      "metadata": {
        "id": "fmDTUNn1KDdW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Training one instance of DQN\n",
        "# dqn_agent = DQNAgent(\n",
        "#     env=(get_dqn_env, dict()),  # we can send (constructor, kwargs) as an env\n",
        "#     **DQN_PARAMS\n",
        "# )\n",
        "# dqn_agent.fit(DQN_TRAINING_TIMESTEPS)\n",
        "\n",
        "#\n",
        "# Training several instances using AgentManager\n",
        "#\n",
        "manager_kwargs = dict(\n",
        "    agent_class=DQNAgent,\n",
        "    train_env=(get_dqn_env, dict()),\n",
        "    eval_env=(get_dqn_env, dict()),\n",
        "    fit_budget=DQN_TRAINING_TIMESTEPS,\n",
        "    n_fit=2,                   # NOTE: You may increase this parameter (number of agents to train)\n",
        "    parallelization='thread',\n",
        "    seed=456,\n",
        "    default_writer_kwargs=dict(maxlen=None,log_interval=10),\n",
        ")"
      ],
      "metadata": {
        "id": "2vCE9FwyKGhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dqn_manager = AgentManager(\n",
        "    init_kwargs=DQN_PARAMS,\n",
        "    agent_name='DQN',\n",
        "    **manager_kwargs\n",
        ")\n",
        "dqn_manager.fit()\n",
        "\n"
      ],
      "metadata": {
        "id": "Y6_SyDibKPFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_dqn_managers = []\n",
        "all_dqn_managers.append(dqn_manager)\n",
        "\n",
        "# We can plot the data that was\n",
        "# stored by the agent with self.writer.add_scalar(tag, value, global_step):\n",
        "_ = plot_writer_data(all_dqn_managers, tag='q_loss', title='Q Loss')\n",
        "_ = plot_writer_data(all_dqn_managers, tag='episode_rewards', title='Rewards (Evaluation)')"
      ],
      "metadata": {
        "id": "f8hNtu60KTTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = dqn_manager.get_agent_instances()[0]\n",
        "render_dqn_policy(agent)"
      ],
      "metadata": {
        "id": "ToqvydT_2_yk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}