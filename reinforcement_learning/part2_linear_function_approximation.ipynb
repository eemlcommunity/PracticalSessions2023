{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "private_outputs": true,
      "authorship_tag": "ABX9TyM4zh3ZUpPeXw4+RNoIOTp5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eemlcommunity/PracticalSessions2023/blob/omardd%2Frl/reinforcement_learning/part2_linear_function_approximation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [EEML 2023] Reinforcement Learning Tutorial - Part 2\n",
        "\n",
        "## RL with Linear Function Approximation"
      ],
      "metadata": {
        "id": "U8ofJ70nEDlb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Colab Setup"
      ],
      "metadata": {
        "id": "FKmrDfGsGiTI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYpbm_8ODqNi"
      },
      "outputs": [],
      "source": [
        "# Colab setup\n",
        "from IPython import get_ipython\n",
        "\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "  # install rlberry library (https://github.com/rlberry-py/rlberry)\n",
        "  !pip install rlberry==0.5.0 > /dev/null 2>&1\n",
        "\n",
        "  # install ffmpeg-python for saving videos\n",
        "  !pip install ffmpeg-python > /dev/null 2>&1\n",
        "\n",
        "  # packages required to show video\n",
        "  !pip install pyvirtualdisplay > /dev/null 2>&1\n",
        "  !apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "\n",
        "# Check rlberry version\n",
        "import rlberry\n",
        "print(rlberry.__version__)\n",
        "\n",
        "# Create directory for saving videos\n",
        "!mkdir videos > /dev/null 2>&1\n",
        "\n",
        "# Initialize display and import function to show videos\n",
        "import rlberry.colab_utils.display_setup\n",
        "from rlberry.colab_utils.display_setup import show_video"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "from rlberry.envs import GridWorld"
      ],
      "metadata": {
        "id": "dFp64jc6GO1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def render_policy(env, policy=None, horizon=50):\n",
        "  \"\"\"Visualize a policy in an environment\n",
        "\n",
        "  Args:\n",
        "    env: GridWorld\n",
        "        environment where to run the policy\n",
        "    policy: np.array\n",
        "        matrix mapping states to action (Ns).\n",
        "        If None, runs random policy.\n",
        "    horizon: int\n",
        "        maximum number of timesteps in the environment.\n",
        "  \"\"\"\n",
        "  env.enable_rendering()\n",
        "  state, info = env.reset()                       # get initial state\n",
        "  for timestep in range(horizon):\n",
        "      if policy is None:\n",
        "        action = env.action_space.sample()  # take random actions\n",
        "      else:\n",
        "        action = policy[state]\n",
        "      next_state, reward, terminated, truncated, info = env.step(action)\n",
        "      state = next_state\n",
        "      if terminated or truncated:\n",
        "        break\n",
        "  # save video and clear buffer\n",
        "  env.save_video('./videos/gw.mp4', framerate=5)\n",
        "  env.clear_render_buffer()\n",
        "  env.disable_rendering()\n",
        "  # show video\n",
        "  show_video('./videos/gw.mp4')"
      ],
      "metadata": {
        "id": "SF3E8p1SHKug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Map"
      ],
      "metadata": {
        "id": "7C-uLPBlG54Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_large_gridworld():\n",
        "  \"\"\"Creates an instance of a grid-world MDP with more states.\"\"\"\n",
        "  walls = [(ii, 10) for ii in range(15) if (ii != 7 and ii != 8)]\n",
        "  env = GridWorld(\n",
        "      nrows=15,\n",
        "      ncols=15,\n",
        "      reward_at = {(14, 14):1.0},\n",
        "      walls=tuple(walls),\n",
        "      success_probability=0.9,\n",
        "      terminal_states=((14, 14),)\n",
        "  )\n",
        "  return env\n",
        "\n",
        "\n",
        "class GridWorldFeatureMap:\n",
        "  \"\"\"Create features for state-action pairs.\n",
        "\n",
        "  Creates features based on the factorization of a matrix\n",
        "  containing similarities between states.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  dim: int\n",
        "    Feature dimension\n",
        "  sigma: float\n",
        "    RBF kernel bandwidth\n",
        "  \"\"\"\n",
        "  def __init__(self, env, dim=15, sigma=0.25):\n",
        "    self.index2coord = env.index2coord\n",
        "    self.n_states = env.Ns\n",
        "    self.n_actions = env.Na\n",
        "    self.dim = dim\n",
        "    self.sigma = sigma\n",
        "\n",
        "    n_rows = env.nrows\n",
        "    n_cols = env.ncols\n",
        "\n",
        "    # build similarity matrix\n",
        "    sim_matrix = np.zeros((self.n_states, self.n_states))\n",
        "    for ii in range(self.n_states):\n",
        "        row_ii, col_ii = self.index2coord[ii]\n",
        "        x_ii = row_ii / n_rows\n",
        "        y_ii = col_ii / n_cols\n",
        "        for jj in range(self.n_states):\n",
        "            row_jj, col_jj = self.index2coord[jj]\n",
        "            x_jj = row_jj / n_rows\n",
        "            y_jj = col_jj / n_cols\n",
        "            dist = np.sqrt((x_jj - x_ii) ** 2.0 + (y_jj - y_ii) ** 2.0)\n",
        "            sim_matrix[ii, jj] = np.exp(-(dist / sigma) ** 2.0)\n",
        "\n",
        "    # factorize similarity matrix to obtain features\n",
        "    uu, ss, vh = np.linalg.svd(sim_matrix, hermitian=True)\n",
        "    self.feats = vh[:dim, :]\n",
        "\n",
        "  def map(self, observation):\n",
        "    feat = self.feats[:, observation].copy()\n",
        "    return feat"
      ],
      "metadata": {
        "id": "pkeuNjtDHCb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = get_large_gridworld()\n",
        "feat_map = GridWorldFeatureMap(env)\n",
        "\n",
        "# Visualize large gridworld\n",
        "render_policy(env)\n",
        "\n",
        "# The features have dimension (feature_dim).\n",
        "feature_example = feat_map.map(1) # feature representation of s=1\n",
        "print(feature_example)\n",
        "\n",
        "# Initial vector theta representing the Q function\n",
        "theta = np.zeros((feat_map.dim, env.action_space.n))\n",
        "print(theta.shape)\n",
        "print(feature_example @ theta) # approximation of Q(s=1, a)"
      ],
      "metadata": {
        "id": "nFfIBGZpHDbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Collection Strategies"
      ],
      "metadata": {
        "id": "ZO6BebgvGvqD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_random_policy_dataset(env, n_samples):\n",
        "  \"\"\"Get a dataset following a random policy to collect data.\"\"\"\n",
        "  states = []\n",
        "  actions = []\n",
        "  rewards = []\n",
        "  next_states = []\n",
        "\n",
        "  state, _ = env.reset()\n",
        "  for _ in range(n_samples):\n",
        "    action = env.action_space.sample()\n",
        "    next_state, reward, terminated, truncated, info = env.step(action)\n",
        "    states.append(state)\n",
        "    actions.append(action)\n",
        "    rewards.append(reward)\n",
        "    next_states.append(next_state)\n",
        "    # update state\n",
        "    state = next_state\n",
        "    if terminated or truncated:\n",
        "      state, _ = env.reset()\n",
        "\n",
        "  dataset = (states, actions, rewards, next_states)\n",
        "  return dataset\n",
        "\n",
        "def get_uniform_dataset(env, n_samples):\n",
        "  \"\"\"Get a dataset by uniformly sampling states and actions.\"\"\"\n",
        "  states = []\n",
        "  actions = []\n",
        "  rewards = []\n",
        "  next_states = []\n",
        "  for _ in range(n_samples):\n",
        "    state = env.observation_space.sample()\n",
        "    action = env.action_space.sample()\n",
        "    next_state, reward, terminated, truncated, info = env.sample(state, action)\n",
        "    states.append(state)\n",
        "    actions.append(action)\n",
        "    rewards.append(reward)\n",
        "    next_states.append(next_state)\n",
        "\n",
        "  dataset = (states, actions, rewards, next_states)\n",
        "  return dataset\n",
        "\n",
        "\n",
        "# Collect two different datasets\n",
        "num_samples = 1000\n",
        "env = get_large_gridworld()\n",
        "dataset_1 = get_random_policy_dataset(env, num_samples)\n",
        "dataset_2 = get_uniform_dataset(env, num_samples)"
      ],
      "metadata": {
        "id": "qtuRbO6DGy7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fitted Q-Iteration\n",
        "\n",
        "Given a datset $(s_i, a_i, r_i, s_i')$ of (states, actions, rewards, next states), the Fitted Q-Iteration (FQI) algorithm proceeds as follows:\n",
        "\n",
        "\n",
        "* We start from a $Q$ function $Q_0 \\in \\mathcal{F}$, where $\\mathcal{F}$ is a function space;\n",
        "* At every iteration $k$, we compute $Q_{k+1}$ as:\n",
        "\n",
        "$$\n",
        "Q_{k+1}\\in\\arg\\min_{f\\in\\mathcal{F}} \\frac{1}{2}\\sum_{i=1}^N\n",
        "\\left(\n",
        "  f(s_i, a_i) - y_i^k\n",
        "\\right)^2 + \\lambda \\Omega(f)\n",
        "$$\n",
        "where $y_i^k = r_i + \\gamma \\max_{a'}Q_k(s_i', a')$, $\\Omega(f)$ is a regularization term and $\\lambda > 0$ is the regularization coefficient.\n",
        "\n",
        "\n",
        "Consider FQI with *linear* function approximation. That is, for a given feature map $\\phi : S \\rightarrow \\mathbb{R}^d$, we consider a parametric family of $Q$ functions $Q_\\theta(s,a) = \\phi(s)^T\\theta_a$ for $\\theta_a\\in\\mathbb{R}^d$. Suppose we are applying FQI on a given dataset of $N$ tuples of the form $(s_i, a_i, r_i, s_i')$ and we are at the $k$-th iteration. Let $\\theta_k \\in\\mathbb{R}^{d \\times A}$ be our current parameter. Derive the *closed-form* update to find $\\theta_{k+1}$, using $\\frac{1}{2}\\sum_a ||\\theta_a||_2^2$ as regularization.\n",
        "\n",
        "\n",
        "Implement Linear FQI in the function below.\n",
        "\n",
        "### What you need to implement\n",
        "\n",
        "The solution of the linear system below:\n",
        "\n",
        "$$\n",
        "(M_a + \\lambda I) \\theta_a = b_a\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "* $M_a = \\sum_{i} \\mathbb{1}\\{a_i = a\\}\\phi(s_i)\\phi(s_i)^T$ is a (d, d) matrix.\n",
        "* $b_a = \\sum_{i} \\mathbb{1}\\{a_i = a\\}\\phi(s_i) y_i$ is a (d, 1) matrix.\n",
        "* $y_i = r_i + \\gamma \\max_{a'} \\phi(s_i', a)^T \\theta_a $\n",
        "* $\\mathbb{1}\\{a_i = a\\}$ is 1 if $a_i=a$ and 0 otherwise.\n",
        "\n",
        "\n",
        "Notice that $M_a$ does not change every iteration, whereas $b_a$ does (due to the changing targets $y_i$).\n"
      ],
      "metadata": {
        "id": "s4yOCBffHekQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_fqi(env, feat_map, num_iterations):\n",
        "  # get a dataset\n",
        "  dataset = get_uniform_dataset(env, n_samples=10000)\n",
        "\n",
        "  # parameters\n",
        "  lambd = 0.1\n",
        "  gamma = 0.95\n",
        "\n",
        "  theta = np.zeros((feat_map.dim, env.Na))\n",
        "\n",
        "  # design matrix M_a\n",
        "  M = np.zeros((env.Na, feat_map.dim, feat_map.dim))\n",
        "  for state, action, reward, next_state in zip(*dataset):\n",
        "    state_feat = feat_map.map(state).reshape(-1, 1)\n",
        "    M[action] += state_feat @ state_feat.T\n",
        "\n",
        "  for it in tqdm(range(num_iterations), desc=\"running linear FQI\"):\n",
        "    # build targets for linear regression\n",
        "\n",
        "    b = np.zeros((env.Na, feat_map.dim))\n",
        "    for state, action, reward, next_state in zip(*dataset):\n",
        "      state_feat = feat_map.map(state)\n",
        "      next_state_feat = feat_map.map(next_state)\n",
        "      # ====================================================\n",
        "      # YOUR IMPLEMENTATION HERE - y_i\n",
        "      target = 0 # ...\n",
        "      # ====================================================\n",
        "      b[action] += state_feat * target\n",
        "    # update theta\n",
        "    for aa in range(env.Na):\n",
        "      # ====================================================\n",
        "      # YOUR IMPLEMENTATION HERE - solving the linear system\n",
        "      pass\n",
        "      # theta[:, aa] = ...\n",
        "      # ====================================================\n",
        "  return theta\n",
        "\n",
        "\n",
        "# environment and feature map\n",
        "env = get_large_gridworld()\n",
        "# env = get_env()\n",
        "feat_map = GridWorldFeatureMap(env, dim=100, sigma=0.2)\n",
        "\n",
        "# FQI\n",
        "theta = linear_fqi(env, feat_map, num_iterations=100)\n",
        "\n",
        "# Compute and run greedy policy\n",
        "Q_fqi = np.zeros((env.Ns, env.Na))\n",
        "for ss in range(env.Ns):\n",
        "  state_feat = feat_map.map(ss)\n",
        "  Q_fqi[ss, :] = state_feat @ theta\n",
        "\n",
        "V_fqi = Q_fqi.max(axis=1)\n",
        "policy = Q_fqi.argmax(axis=1)\n",
        "render_policy(env, policy, horizon=100)\n",
        "img = env.get_layout_img(V_fqi)\n",
        "plt.imshow(img)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9OSBpc54HjRu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}