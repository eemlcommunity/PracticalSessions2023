{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "private_outputs": true,
      "authorship_tag": "ABX9TyNvO++Z1l8ggGlxBc8EOT/u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eemlcommunity/PracticalSessions2023/blob/omardd%2Frl/reinforcement_learning/introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [EEML 2023] RL Tutorial - Introduction\n",
        "\n"
      ],
      "metadata": {
        "id": "14Rh_njw1zrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In reinforcement learning (RL), we are interested in learning **policies** that allow an agent to **act optimally** in an **environment**.\n",
        "\n",
        "There are two fundamental concepts in RL:\n",
        "\n",
        "* policies $\\pi(s, a)$ that prescribes with action $a$ to take in a state $s$;\n",
        "* value functions $Q(s, a)$ that represent the value of taking action $a$ in a state $s$.\n",
        "\n",
        "An optimal policy $\\pi^*$ can be obtained from the optimal Q function $Q^*$ by selecting in each state $s$ the action maximizing $Q^*(s, a)$.\n",
        "\n",
        "\n",
        "This tutorial focuses on **value-based** algorithms, i.e., those aiming to approximate $Q^*$."
      ],
      "metadata": {
        "id": "NDBvnVYt2I1l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interacting with an environment"
      ],
      "metadata": {
        "id": "vywdlNMe3NgG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to use the [Gymnasium API](https://gymnasium.farama.org/) to create and interact with RL environments in Python."
      ],
      "metadata": {
        "id": "4TNIeqmx7HI6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q swig > /dev/null 2>&1\n",
        "!pip install \"gymnasium[box2d]\" > /dev/null 2>&1\n",
        "\n",
        "# install rlberry library (https://github.com/rlberry-py/rlberry)\n",
        "!pip install rlberry==0.5.0 > /dev/null 2>&1\n",
        "# install ffmpeg-python for saving videos\n",
        "!pip install ffmpeg-python > /dev/null 2>&1\n",
        "# packages required to show video\n",
        "!pip install pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n"
      ],
      "metadata": {
        "id": "6K59rbUh77pj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env_id = \"LunarLander-v2\" #@param"
      ],
      "metadata": {
        "id": "fEpGCOWjHpd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "\n",
        "env = gym.make(env_id)\n",
        "observation, info = env.reset(seed=42)\n",
        "for _ in range(1000):\n",
        "   action = env.action_space.sample()  # this is where you would insert your policy\n",
        "   observation, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "   if terminated or truncated:\n",
        "      observation, info = env.reset()\n",
        "\n",
        "print(observation, reward, terminated, truncated, info)"
      ],
      "metadata": {
        "id": "lVQyDISy3S48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize display and import function to show videos\n",
        "from gymnasium.utils.save_video import save_video\n",
        "import rlberry.colab_utils.display_setup\n",
        "from rlberry.colab_utils.display_setup import show_video\n",
        "\n",
        "def get_env(for_render=False):\n",
        "  if not for_render:\n",
        "    return gym.make(env_id)\n",
        "  else:\n",
        "    return gym.make(env_id, render_mode=\"rgb_array_list\")\n",
        "\n",
        "def render_random_policy():\n",
        "  env = get_env(for_render=True)\n",
        "  state, _ = env.reset()\n",
        "  step_starting_index = 0\n",
        "  episode_index = 0\n",
        "  for step_index in range(500):\n",
        "    if episode_index > 0: # show only one episode\n",
        "      break\n",
        "    # random action\n",
        "    action = env.action_space.sample()\n",
        "    next_state, reward, terminated, truncated, info = env.step(action)\n",
        "    # end of episode\n",
        "    if terminated or truncated:\n",
        "        save_video(\n",
        "          env.render(),\n",
        "          \"videos\",\n",
        "          fps=env.metadata[\"render_fps\"],\n",
        "          step_starting_index=step_starting_index,\n",
        "          episode_index=episode_index\n",
        "        )\n",
        "        step_starting_index = step_index + 1\n",
        "        episode_index += 1\n",
        "        next_state, _ = env.reset()\n",
        "    state = next_state\n",
        "  show_video(\"videos/rl-video-episode-0.mp4\")\n",
        "\n",
        "\n",
        "render_random_policy()"
      ],
      "metadata": {
        "id": "rp4s6yyvHVmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Review of Main Concepts"
      ],
      "metadata": {
        "id": "mjGftZWbcIt3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Markov Decision Processes and Value Functions\n",
        "\n",
        "In reinforcement learning, an agent interacts with an enviroment by taking actions and observing rewards. Its goal is to learn a *policy*, that is, a mapping from states to actions, that maximizes the amount of reward it gathers.\n",
        "\n",
        "The enviroment is modeled as a __Markov decision process (MDP)__, defined by a set of states $\\mathcal{S}$, a set of actions $\\mathcal{A}$, a reward function $r(s, a)$ and transition probabilities $P(s'|s,a)$. When an agent takes action $a$ in state $s$, it receives a random reward with mean $r(s,a)$ and makes a transion to a state $s'$ distributed according to $P(s'|s,a)$.\n",
        "\n",
        "A __policy__ $\\pi$ is such that $\\pi(a|s)$ gives the probability of choosing an action $a$ in state $s$. __If the policy is deterministic__, we denote by $\\pi(s)$ the action that it chooses in state $s$. We are interested in finding a policy that maximizes the value function $V^\\pi$, defined as\n",
        "\n",
        "$$\n",
        "V^\\pi(s) = \\sum_{a\\in \\mathcal{A}} \\pi(a|s) Q^\\pi(s, a),\n",
        "\\quad \\text{where} \\quad\n",
        "Q^\\pi(s, a) = \\mathbf{E}\\left[ \\sum_{t=0}^\\infty \\gamma^t r(S_t, A_t)  \\Big| S_0 = s, A_0 = a\\right].\n",
        "$$\n",
        "and represents the mean of the sum of discounted rewards gathered by the policy $\\pi$ in the MDP, where $\\gamma \\in [0, 1[$ is a discount factor ensuring the convergence of the sum.\n",
        "\n",
        "The __action-value function__ $Q^\\pi$ is the __fixed point of the Bellman operator $T^\\pi$__:\n",
        "\n",
        "$$\n",
        "Q^\\pi(s, a) = T^\\pi Q^\\pi(s, a)\n",
        "$$\n",
        "where, for any function $f: \\mathcal{S}\\times\\mathcal{A} \\to \\mathbb{R}$\n",
        "$$\n",
        "T^\\pi f(s, a) =  r(s, a) + \\gamma \\sum_{s'} P(s'|s,a) \\left(\\sum_{a'}\\pi(a'|s')f(s',a')\\right)\n",
        "$$\n",
        "\n",
        "\n",
        "The __optimal value function__, defined as $V^*(s) = \\max_\\pi V^\\pi(s)$ can be shown to satisfy $V^*(s) = \\max_a Q^*(s, a)$, where $Q^*$ is the __fixed point of the optimal Bellman operator $T^*$__:\n",
        "\n",
        "$$\n",
        "Q^*(s, a) = T^* Q^*(s, a)\n",
        "$$\n",
        "where, for any function $f: \\mathcal{S}\\times\\mathcal{A} \\to \\mathbb{R}$\n",
        "$$\n",
        "T^* f(s, a) =  r(s, a) + \\gamma \\sum_{s'} P(s'|s,a) \\max_{a'} f(s', a')\n",
        "$$\n",
        "and there exists an __optimal policy__ which is deterministic, given by $\\pi^*(s) \\in \\arg\\max_a Q^*(s, a)$.\n",
        "\n"
      ],
      "metadata": {
        "id": "JcujClR7cFMA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Value iteration\n",
        "\n",
        "If both the reward function $r$ and the transition probablities $P$ are known, we can compute $Q^*$ using value iteration, which proceeds as follows:\n",
        "\n",
        "1. Start with arbitrary $Q_0$, set $t=0$.\n",
        "2. Compute $Q_{t+1}(s, a) = T^*Q_t(s,a)$ for every $(s, a)$.\n",
        "3. If $\\max_{s,a} | Q_{t+1}(s, a) -  Q_t(s,a)| \\leq \\varepsilon$, return $Q_{t}$. Otherwise, set $t \\gets t+1$ and go back to 2.\n",
        "\n",
        "The convergence is guaranteed by the contraction property of the Bellman operator, and $Q_{t+1}$ can be shown to be a good approximation of $Q^*$ for small epsilon.\n",
        "\n",
        "__Question__: Can you bound the error $\\max_{s,a} | Q^*(s, a) -  Q_t(s,a)|$ as a function of $\\gamma$ and $\\varepsilon$?\n"
      ],
      "metadata": {
        "id": "FkISFdGncV2i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q-Learning\n",
        "\n",
        "In value iteration, we need to know $r$ and $P$ to implement the Bellman operator. When these quantities are not available, we can approximate $Q^*$ using *samples* from the environment with the Q-Learning algorithm.\n",
        "\n",
        "Q-Learning with __$\\varepsilon$-greedy exploration__ proceeds as follows:\n",
        "\n",
        "1. Start with arbitrary $Q_0$, get starting state $s_0$, set $t=0$.\n",
        "2. Choosing action $a_t$:\n",
        "  * With probability $\\varepsilon$ choose $a_t$ randomly (uniform distribution)  \n",
        "  * With probability $1-\\varepsilon$, choose $a_t \\in \\arg\\max_a Q_t(s_t, a)$.\n",
        "3. Take action $a_t$, observe next state $s_{t+1}$ and reward $r_t$.\n",
        "4. Compute error $\\delta_t = r_t + \\gamma \\max_a Q_t(s_{t+1}, a) - Q_t(s_t, a_t)$.\n",
        "5. Update\n",
        "  * $Q_{t+1}(s, a) = Q_t(s, a) + \\alpha_t(s,a) \\delta_t$,  __if $s=s_t$ and $a=a_t$__\n",
        "  * $Q_{t+1}(s, a) = Q_{t}(s, a)$ otherwise.\n",
        "\n",
        "Here, $\\alpha_t(s,a)$ is a learning rate that can depend, for instance, on the number of times the algorithm has visited the state-action pair $(s, a)$."
      ],
      "metadata": {
        "id": "zCiNwkMHcYAS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fitted Q-Iteration\n",
        "\n",
        "Given a datset $(s_i, a_i, r_i, s_i')$ of (states, actions, rewards, next states), the Fitted Q-Iteration (FQI) algorithm proceeds as follows:\n",
        "\n",
        "\n",
        "* We start from a $Q$ function $Q_0 \\in \\mathcal{F}$, where $\\mathcal{F}$ is a function space;\n",
        "* At every iteration $k$, we compute $Q_{k+1}$ as:\n",
        "\n",
        "$$\n",
        "Q_{k+1}\\in\\arg\\min_{f\\in\\mathcal{F}} \\frac{1}{2}\\sum_{i=1}^N\n",
        "\\left(\n",
        "  f(s_i, a_i) - y_i^k\n",
        "\\right)^2 + \\lambda \\Omega(f)\n",
        "$$\n",
        "where $y_i^k = r_i + \\gamma \\max_{a'}Q_k(s_i', a')$, $\\Omega(f)$ is a regularization term and $\\lambda > 0$ is the regularization coefficient.\n",
        "\n",
        "\n",
        "Consider FQI with *linear* function approximation. That is, for a given feature map $\\phi : S \\rightarrow \\mathbb{R}^d$, we consider a parametric family of $Q$ functions $Q_\\theta(s,a) = \\phi(s)^T\\theta_a$ for $\\theta_a\\in\\mathbb{R}^d$. Suppose we are applying FQI on a given dataset of $N$ tuples of the form $(s_i, a_i, r_i, s_i')$ and we are at the $k$-th iteration. Let $\\theta_k \\in\\mathbb{R}^{d \\times A}$ be our current parameter. The *closed-form* update to find $\\theta_{k+1}$, using $\\frac{1}{2}\\sum_a ||\\theta_a||_2^2$ as regularization is found by solving the linear system:\n",
        "\n",
        "\n",
        "$$\n",
        "(M_a + \\lambda I) \\theta_a = b_a\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "* $M_a = \\sum_{i} \\mathbb{1}\\{a_i = a\\}\\phi(s_i)\\phi(s_i)^T$ is a (d, d) matrix.\n",
        "* $b_a = \\sum_{i} \\mathbb{1}\\{a_i = a\\}\\phi(s_i) y_i$ is a (d, 1) matrix.\n",
        "* $y_i = r_i + \\gamma \\max_{a'} \\phi(s_i', a)^T \\theta_a $\n",
        "* $\\mathbb{1}\\{a_i = a\\}$ is 1 if $a_i=a$ and 0 otherwise."
      ],
      "metadata": {
        "id": "DBFr812M54no"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deep Q-Learning\n",
        "\n",
        "\n",
        "Deep Q-Learning uses a neural network to approximate $Q$ functions. Hence, we usually refer to this algorithm as DQN (for *deep Q network*).\n",
        "\n",
        "The parameters of the neural network are denoted by $\\theta$.\n",
        "*   As input, the network takes a state $s$,\n",
        "*   As output, the network returns $Q(s, a, \\theta)$, the value of each action $a$ in state $s$, according to the parameters $\\theta$.\n",
        "\n",
        "\n",
        "The goal of Deep Q-Learning is to learn the parameters $\\theta$ so that $Q(s, a, \\theta)$ approximates well the optimal $Q$-function $Q^*(s, a)$.\n",
        "\n",
        "In addition to the network with parameters $\\theta$, the algorithm keeps another network with the same architecture and parameters $\\theta^-$, called **target network**.\n",
        "\n",
        "The algorithm works as follows:\n",
        "\n",
        "1.   At each time $t$, the agent is in state $s_t$ and has observed the transitions $(s_i, a_i, r_i, s_i')_{i=1}^{t-1}$, which are stored in a **replay buffer**.\n",
        "\n",
        "2.  Choose action $a_t = \\arg\\max_a Q(s_t, a)$ with probability $1-\\varepsilon_t$, and $a_t$=random action with probability $\\varepsilon_t$.\n",
        "\n",
        "3. Take action $a_t$, observe reward $r_t$ and next state $s_t'$.\n",
        "\n",
        "4. Add transition $(s_t, a_t, r_t, s_t')$ to the **replay buffer**.\n",
        "\n",
        "4.  Sample a minibatch $\\mathcal{B}$ containing $B$ transitions from the replay buffer. Using this minibatch, we define the loss:\n",
        "\n",
        "$$\n",
        "L(\\theta) = \\sum_{(s_i, a_i, r_i, s_i') \\in \\mathcal{B}}\n",
        "\\left[\n",
        "Q(s_i, a_i, \\theta) -  y_i\n",
        "\\right]^2\n",
        "$$\n",
        "where the $y_i$ are the **targets** computed with the **target network** $\\theta^-$:\n",
        "\n",
        "$$\n",
        "y_i = r_i + \\gamma \\max_{a'} Q(s_i', a', \\theta^-).\n",
        "$$\n",
        "\n",
        "5. Update the parameters $\\theta$ to minimize the loss, e.g., with gradient descent (**keeping $\\theta^-$ fixed**):\n",
        "$$\n",
        "\\theta \\gets \\theta - \\eta \\nabla_\\theta L(\\theta)\n",
        "$$\n",
        "where $\\eta$ is the optimization learning rate.\n",
        "\n",
        "6. Every $N$ transitions ($t\\mod N$ = 0), update target parameters: $\\theta^- \\gets \\theta$.\n",
        "\n",
        "7. $t \\gets t+1$. Stop if $t = T$, otherwise go to step 2."
      ],
      "metadata": {
        "id": "591xHbsJ5_Pl"
      }
    }
  ]
}