{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eemlcommunity/PracticalSessions2023/blob/main/eeml_2023_intro_full.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8yw_hrKYuL6"
      },
      "source": [
        "# Introduction to Colab, JAX, haiku\n",
        "\n",
        "Authors: Pavol Drotar\n",
        "\n",
        "Adopted from: David Szepesvari, Viorica Patraucean\n",
        "\n",
        "Contact: pavol.drotar3@gmail.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsr0IKnOYEUi"
      },
      "source": [
        "## What is Colab?\n",
        "\n",
        "[Colaboratory](https://colab.sandbox.google.com/notebooks/welcome.ipynb) is a [Jupyter](http://jupyter.org/) notebook environment that requires no setup to use. It allows you to create and share documents that contain\n",
        "\n",
        "* Live, runnable code\n",
        "* Visualizations\n",
        "* Explanatory text\n",
        "\n",
        "It's also a great tool for prototyping and quick development. Let's give it a try."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZjpiZSDYJ1R"
      },
      "source": [
        "Run the following *(Code) Cell* hitting **`Shift + Enter`** inside it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIRY1OxWYNED"
      },
      "source": [
        "print('Hello EEML!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okXieYk8YSZ5"
      },
      "source": [
        "You should see the `Hello EEML!` printed under the code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXzTe0-8Yk12"
      },
      "source": [
        "### Using a GPU\n",
        "\n",
        "Code is running on a VM and results are sent to your browser. You can connect to a VM with a GPU using:\n",
        "\n",
        "* **Runtime > Change runtime type**\n",
        "* **Hardware Accelerator > GPU**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USfPlwlyY0oK"
      },
      "source": [
        "### Losing Connection\n",
        "\n",
        "You may lose connection to your virtual machine. The two most common causes are\n",
        "\n",
        "* Virtual machines are recycled when idle for a while, and have a maximum lifetime enforced by the system.\n",
        "* Long-running background computations, particularly on GPUs, may be stopped.\n",
        "\n",
        "**If you lose connection**, the state of your notebook will also be lost. You will need to **rerun all cells** up to the one you are currently working on. To do so\n",
        "\n",
        "1. Select (place the cursor into) the cell you are working on.\n",
        "2. Follow **Runtime > Run before**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWgJEOspaQA_"
      },
      "source": [
        "### Pretty Printing by colab\n",
        "1) If the **last operation** of a given cell returns a value, it will be pretty printed by colab.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttk-hcG2aT7M"
      },
      "source": [
        "6 * 7"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8hMw-vRaXP1"
      },
      "source": [
        "my_dict = {'one': 1, 'some set': {4, 2, 2}, 'a regular list': range(5)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNs8mVyYaa-n"
      },
      "source": [
        "There is no output from the second cell, as assignment does not return anything."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yg4BV2bXaeqT"
      },
      "source": [
        "2) You can explicitly **print** anything before the last operation, or **supress** the output of the last operation by adding a semicolon."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mn0aHa_pajOT"
      },
      "source": [
        "print(my_dict)\n",
        "my_dict['one'] * 10 + 1;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFVpqvB-ZTot"
      },
      "source": [
        "### Scoping and Execution Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdeQn9ExapJM"
      },
      "source": [
        "Notice that in the previous code cell we worked with `my_dict`, while it was defined in an even earlier cell.\n",
        "\n",
        "1) In colabs, variables defined at cell root have **global** scope.\n",
        "\n",
        "Modify `my_dict`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wx4_0k3Ka6KR"
      },
      "source": [
        "my_dict['I\\'ve been changed!'] = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsXP0Spya_NG"
      },
      "source": [
        "2) Cells can be **run** in any **arbitrary order**, and global state is maintained between them.\n",
        "\n",
        "Try re-running the cell where we printed `my_dict`. You should see now  see the additional item `\"I've been changed!\": True`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFofwmqAZP6s"
      },
      "source": [
        "3) Unintentionally reusing a global variable can lead to bugs. If all else fails, you can uncomment and run the following line to **clear all global variables** and run again all the cells."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugbgF-kCZoE-"
      },
      "source": [
        "# %reset -f"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### IPython magic\n",
        " The function above was prefixed with `%`. These are called IPython magic functions and apply to a single line. Cell magics work on entire cells and are prefixed with double `%%`."
      ],
      "metadata": {
        "id": "KhJyhbFHx8el"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You can use %timeit to time lines or cells\n",
        "%timeit [n ** 2 for n in range(100)]"
      ],
      "metadata": {
        "id": "a0g3xzTUy9Yg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "s = 0\n",
        "for i in range(100):\n",
        "  s = s + i\n",
        "print(s)"
      ],
      "metadata": {
        "id": "iBKtFYp2zJ3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use %magic to get information about available magic commands\n",
        "%magic"
      ],
      "metadata": {
        "id": "w5Qx1tl3z02u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRWQOmFKbSNW"
      },
      "source": [
        "### Setup and Imports\n",
        "\n",
        "Python packages can and need to be imported into your colab notebook, the same way you would import them in a python script. For example, to use `numpy`, you would do"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLiqqd44bg3B"
      },
      "source": [
        "# import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9_Kj04gbXsf"
      },
      "source": [
        "While many packages can just be imported, some (e.g. `haiku`, a neural network library from DeepMind) may not be prepackaged in the runtime. With Colab, you can install any python package from `pip` for the duration of your connection."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAmMS5h8bn_1"
      },
      "source": [
        "# we will use haiku on top of jax\n",
        "# !pip install -q dm-haiku\n",
        "# import haiku as hk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXYkeehcb8kg"
      },
      "source": [
        "### Forms\n",
        "\n",
        "With colab it is easy to take input from the user in code cells through so called forms. A simplest example is shown below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAZhaY6NcXlc"
      },
      "source": [
        "#@title This text shows up as a title.\n",
        "\n",
        "a = 2  #@param {type: 'integer'}\n",
        "b = 3  #@param\n",
        "\n",
        "print(f'a+b = {str(a+b)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDlY63E-ccL7"
      },
      "source": [
        "You can change parameters on the right hand side, then rerun the cell to use these values. **Try setting the value of a=5 and rerun the cell above.**\n",
        "\n",
        "In order to expose a variable as parameter you just add `#@param` after it. There are various kinds of params, if you're interested you can read more about this on the official starting colab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjPF8rKiize7"
      },
      "source": [
        "## JAX\n",
        "[JAX](https://jax.readthedocs.io/en/latest/jax.html) allows NumPy-like code to execute on CPU, or accelerators like GPU, and TPU, with great automatic differentiation for high-performance machine learning research.\n",
        "\n",
        "- JAX automatically differentiates python code and NumPy code (with [Autograd](https://github.com/hips/autograd))\n",
        "- uses [XLA](https://www.tensorflow.org/xla) to compile and run NumPy code efficiently on accelerators\n",
        "\n",
        "This makes JAX a great tool for high-performance numerical computations and machine learning research."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TE4j9mLOQFpF"
      },
      "source": [
        "**Key Concepts:**\n",
        "\n",
        "* JAX provides a NumPy-inspired interface for convenience.\n",
        "* Through duck-typing, JAX arrays can often be used as drop-in replacements of NumPy arrays.\n",
        "* Unlike NumPy arrays, JAX arrays are always immutable.\n",
        "\n",
        "JAX has a functional interface, that is, all functions are pure, with no side effects. [This is what allows](https://colab.research.google.com/github/google/jax/blob/main/docs/jax-101/07-state.ipynb#scrollTo=Avjnyrjojo8z) the transformations/autograd to work.\n",
        "\n",
        "Various neural network libraries have been built on top of JAX to enable fast research and provide more familiar object oriented interfaces. We will see two of these below: haiku and flax."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDmbQoL5mYmA"
      },
      "source": [
        "### JAX and random number generators\n",
        "Unlike many ML frameworks, JAX does not hide the pseudo-random number generator state. You need to generate a random key, and pass it to the operations that work with random numbers (e.g. initialising a model, dropout etc). A call to a random function with the same key does not change the state of the generator. This has to be done explicitly with `split()` or `next_rng_key()` in `haiku`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0j5kLgolmlCl"
      },
      "source": [
        "import numpy as np\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "key = random.PRNGKey(0)\n",
        "# The output is the same if the same key is used\n",
        "x1 = random.normal(key, (3,))\n",
        "x2 = random.normal(key, (3,))\n",
        "print(x1)\n",
        "print(x2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZNs5VAdfmpG"
      },
      "source": [
        "# Let's split the key to be able to generate different random values\n",
        "key, new_key = random.split(key)\n",
        "x1 = random.normal(key, (3,))\n",
        "print (x1)\n",
        "x2 = random.normal(new_key, (3,))\n",
        "print (x2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nwjljk9RhDh"
      },
      "source": [
        "Each time you need to use randomness, split a key and use one for your needs, the other to split later on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hiqCH1Nkvdv"
      },
      "source": [
        "### JAX program transformations with examples\n",
        "* `grad` -- returns derivatives of function with respect to the model weights passed as parameters\n",
        "* `vmap` -- automatic batching; returns a new function that can apply the original (per-sample) function to a batch.\n",
        "* `jit` (just-in-time compilation) -- speeds up your code by running all the ops inside the jit-ed function as a *fused* op; it compiles the function when it's called the first time and uses the compiled (optimised) version from the second call onwards.\n",
        "* `pmap` -- transforms a function to run on multiple devices in parallel and allows some cross device communication (not covered here)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `jax.grad()` function uses autograd to compute gradients of pure functions\n"
      ],
      "metadata": {
        "id": "xMQlhH8n-grL"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rP70-aR3oouX"
      },
      "source": [
        "# Let's use grad to compute gradient of tanh\n",
        "from jax import grad\n",
        "def tanh(x):\n",
        "  # Remember to use jnp instead of np for transformations to work\n",
        "  y = jnp.exp(-2.0 * x)\n",
        "  return (1.0 - y) / (1.0 + y)\n",
        "\n",
        "grad_tanh = grad(tanh)\n",
        "# Take the gradient at x = 1.0\n",
        "print(grad_tanh(1.))\n",
        "\n",
        "# You can also take the 2nd or higher gradients\n",
        "print(grad(grad_tanh)(1.))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FH8uTx364T0m"
      },
      "source": [
        "# Let's plot up to the third gradient\n",
        "import matplotlib.pyplot as plt\n",
        "x = jnp.arange(-8, 8, .1)\n",
        "plt.plot(\n",
        "    x, tanh(x),\n",
        "    x, [grad(tanh)(xi) for xi in x],\n",
        "    x, [grad(grad(tanh))(xi) for xi in x],\n",
        "    x, [grad(grad(grad(tanh)))(xi) for xi in x],\n",
        ")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Note that while tanh works on arrays\n",
        "tanh(x)\n",
        "\n",
        "# If you try passing x to grad(tanh), it will fail\n",
        "# because grad only works for scalar output functions\n",
        "#grad(tanh)(x)\n",
        "\n",
        "# That is why we evaluated the gradient element-wise,\n",
        "# which is somewhat inefficient as you might have noticed at runtime\n",
        "[grad(tanh)(xi) for xi in x]\n"
      ],
      "metadata": {
        "id": "nkEsjylS7eZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foOt9Hq2_dqI"
      },
      "source": [
        "# jax.vmap can be used to vectorize functions efficiently\n",
        "# we can use it to vectorize the abov gradient functions\n",
        "from jax import vmap\n",
        "\n",
        "vectorized_grad_tanh = vmap(grad(tanh))\n",
        "\n",
        "# Plot again and check that the gradients are identical\n",
        "plt.plot(\n",
        "    x, tanh(x),\n",
        "    x, vectorized_grad_tanh(x),\n",
        ")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vectorization is a function transformation optimized for speed, not just a for loop in disguise. We can further use jax.jit() to transform our functions to native XLA code."
      ],
      "metadata": {
        "id": "BO-U6iXP_Kat"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiWR4CPjlc6T"
      },
      "source": [
        "# Let's use jit to speed up a function\n",
        "from jax import jit\n",
        "\n",
        "jit_vectorized_grad_tanh = jit(vectorized_grad_tanh)\n",
        "\n",
        "# Check that this yields the same plot\n",
        "plt.plot(\n",
        "    x, tanh(x),\n",
        "    x, jit_vectorized_grad_tanh(x),\n",
        ")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also use the `@jit` decorator which is just a wrapper that transforms the annotated functions for you automatically."
      ],
      "metadata": {
        "id": "B83GRhlqAsAQ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dibhhYsjphE3"
      },
      "source": [
        "# Now let's measure the speedups!\n",
        "grad_tanh = grad(tanh)\n",
        "\n",
        "# manual vectorization\n",
        "def manual_vectorization(x):\n",
        "  return jnp.stack([grad_tanh(xi) for xi in x])\n",
        "\n",
        "# manual vectorization with jit\n",
        "@jit\n",
        "def manual_vectorization_jit(x):\n",
        "  return jnp.stack([grad_tanh(xi) for xi in x])\n",
        "\n",
        "# vmap vectorization\n",
        "def vmap_vectorization(x):\n",
        "  return vmap(grad_tanh)(x)\n",
        "\n",
        "# vmap vectorization with jit\n",
        "@jit\n",
        "def vmap_vectorization_jit(x):\n",
        "  return vmap(grad_tanh)(x)\n",
        "\n",
        "print('Manual vectorization')\n",
        "%timeit manual_vectorization(x).block_until_ready()\n",
        "print('Manual vectorization with jit')\n",
        "%timeit manual_vectorization_jit(x).block_until_ready()\n",
        "print('Vmap vectorization')\n",
        "%timeit vmap_vectorization(x).block_until_ready()\n",
        "print('Vmap vectorization with jit')\n",
        "%timeit vmap_vectorization_jit(x).block_until_ready()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exciting! The speedup is orders of magnitude here. Oh, seeker of knowledge, let your heart be ensnared by the enchanting allure of Jax. For within its [cryptic algorithms](https://jax.readthedocs.io/en/latest/jax-101/02-jitting.html) lies the key to a universe of wonders yet undiscovered."
      ],
      "metadata": {
        "id": "7AMFwmHJCPgg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise: Plot second derivative of sin(x)/x"
      ],
      "metadata": {
        "id": "Z3YzCtNQngGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax"
      ],
      "metadata": {
        "id": "IBOqijcFqxay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Define sin x over x\n",
        "def sin_x_over_x(x):\n",
        "  return jnp.sin(x)/x\n",
        "\n",
        "# TODO: Take second gradient\n",
        "sin_x_over_x_sec = jit(vmap(grad(grad(sin_x_over_x))))\n",
        "\n",
        "# TODO: Plot it between [-10, 10]\n",
        "x = jnp.arange(-10, 10, 0.5)\n",
        "plt.plot(x, sin_x_over_x_sec(x))"
      ],
      "metadata": {
        "id": "ph5IIdploKmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EdK9zpqUl2H"
      },
      "source": [
        "### JAX Pytrees\n",
        "\n",
        "The jax ecosystem (including flax and haiku) relies on structured, nested data -- [pytrees](https://jax.readthedocs.io/en/latest/pytrees.html).\n",
        "\n",
        "In JAX, a pytree is a container of leaf elements and/or more pytrees. Containers include lists, tuples, and dicts. A leaf element is anything thatâ€™s not a pytree, e.g. a jnp array.\n",
        "\n",
        "`pytree = list[pytree] | tuple[pytree] | dict [pytree] | leaf`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwFnfj4_JiM6"
      },
      "source": [
        "Let's see an example (taken from [this tutorial](https://colab.sandbox.google.com/github/google/jax/blob/master/docs/jax-101/05.1-pytrees.ipynb#scrollTo=9UjxVY9ulSCn)):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjhUrw-NUzfz"
      },
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "example_trees = [\n",
        "    [1, 'a', object()],\n",
        "    (1, (2, 3), ()),\n",
        "    [1, {'k1': 2, 'k2': (3, 4)}, 5],\n",
        "    {'a': 2, 'b': (2, 3)},\n",
        "    jnp.array([1, 2, 3]),\n",
        "]\n",
        "\n",
        "# Let's see how many leaves they have, by using `jax.tree_leaves(pytree)`\n",
        "# to access the flattened leaves of the tree\n",
        "for pytree in example_trees:\n",
        "  leaves = jax.tree_util.tree_leaves(pytree)\n",
        "  print(f\"{repr(pytree):<45} has {len(leaves)} leaves: {leaves}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LInfkv5-Kxr0"
      },
      "source": [
        "Places where you commonly find pytrees are:\n",
        "* Model parameters (e.g. see `get_num_params` function below)\n",
        "* Dataset entries\n",
        "* RL agent observations\n",
        "\n",
        "Check the [tutorial linked above](https://colab.sandbox.google.com/github/google/jax/blob/master/docs/jax-101/05.1-pytrees.ipynb#scrollTo=-h05_PNNhZ-D) for more details and exercises on pytrees."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A heavily used pytree function is [jax.tree_map](https://jax.readthedocs.io/en/latest/_autosummary/jax.tree_util.tree_map.html) that maps a function over leaves, which is frequently used to batch pytrees with identical structure."
      ],
      "metadata": {
        "id": "t9wmw3voI_J1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Batching a lits of pytrees\n",
        "batch_size = 4\n",
        "list_of_pytrees = [{'x': jnp.zeros(10), 'y': jnp.ones(10)}] * batch_size\n",
        "pytree_of_batch = jax.tree_map(lambda *leaves: jnp.stack(leaves), *list_of_pytrees)\n",
        "pytree_of_batch"
      ],
      "metadata": {
        "id": "mBpv1cd8J3Xh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSMUMtokL4rO"
      },
      "source": [
        "### JAX More details\n",
        "- For more details on jax, check out this [collection of eight JAX-101 tutorials](https://jax.readthedocs.io/en/latest/jax-101/index.html).\n",
        "- Check out \"The Sharp Bits\" for [common gotchas](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html) in JAX!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training our first ML model in JAX\n",
        "We will implement gradient descent that will fit the parameters of a simple linear model to noisy data. The example is inspired by the [JAX 101 turorial](https://jax.readthedocs.io/en/latest/jax-101/01-jax-basics.html)"
      ],
      "metadata": {
        "id": "VlJkN2ADNqVk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will generate our noisy dataset using the following linear equation `y = x*w + b + noise`. The task is to regress the parameters `w` and `b` by observing training examples of `x,y`."
      ],
      "metadata": {
        "id": "cZyNvzUGO9K9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating a dataset"
      ],
      "metadata": {
        "id": "Zd5gL9j3c0gK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the params\n",
        "\n",
        "w = 3.  #@param\n",
        "b = -1.  #@param"
      ],
      "metadata": {
        "id": "ofFHZiNobUoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's sample a dataset of 100 data points\n",
        "X = np.random.normal(size=(100,))\n",
        "noise = np.random.normal(scale=0.5, size=(100,))\n",
        "Y = X * w + b + noise\n",
        "plt.scatter(X, Y);"
      ],
      "metadata": {
        "id": "44Ec6lTdOa-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining a model"
      ],
      "metadata": {
        "id": "uP_jUDsYc3VL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Our model is going to be y(x,w,b) = x*w + b\n",
        "# We will store the parameters as a single array params = [w, b]\n",
        "@jit\n",
        "def model(params, x):\n",
        "  \"\"\"Computes wx + b on a batch of input x.\"\"\"\n",
        "  w, b = params\n",
        "  return w * x + b"
      ],
      "metadata": {
        "id": "Ipa-yFG6QIrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise: write your loss function"
      ],
      "metadata": {
        "id": "5ID0xQKpoBGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The loss function of choice for regression on gaussian noise is mean squared error.\n",
        "def loss_fn(params, x, y):\n",
        "  prediction = model(params, x)\n",
        "\n",
        "  # TODO: Implement mean squared error\n",
        "  return jnp.mean((prediction-y)**2)"
      ],
      "metadata": {
        "id": "2iEhlCyQRO4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the model"
      ],
      "metadata": {
        "id": "YdgpZNF6c7QL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We minimize the loss by stepping the parameters\n",
        "# in the direction of steepest loss descent, that is,\n",
        "# the negative gradient of params w.r.t. loss.\n",
        "def update(params, x, y, lr=0.02):\n",
        "  return params - lr * jax.grad(loss_fn)(params, x, y)"
      ],
      "metadata": {
        "id": "a-mqL5aGRiWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Can you speed up the loop by adding jit somewhere else?\n",
        "params = jnp.array([1.,1.]) # initialize w=0, b=0\n",
        "for i in range(42):\n",
        "  params = update(params, X, Y)"
      ],
      "metadata": {
        "id": "HmBed8j6Z5PT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Params should end up close to the hidden params\n",
        "params"
      ],
      "metadata": {
        "id": "c8y0KkAWbBFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's use the model to plot the regression fit\n",
        "plt.scatter(X, Y);\n",
        "plt.plot(X, model(params, X))\n",
        "plt.plot(X,  X * w + b)"
      ],
      "metadata": {
        "id": "hXtuyra-bnOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Where is backpropagation?\n",
        "Coming from pytorch, you might be asking this question. In Pytorch you are used to defining a model and calling `loss.backward()`. This builds the computational graph dynamically, then traverses using the chain rule to get the gradients. Each node computes its gradient w.r.t. its params in an object oriented manner, aligning with Pythonic principles.\n",
        "\n",
        "In JAX, differentiation is achieved through just-in-time compilation. The model function is traced and a static graph is constructed and transformed into its gradient (can use either forward or reverse-mode differentiation). JIT is faster and exposing the gradient allows more powerful parameter updates. However, static graphs are less flexible to dynamic control flow and warying input sizes.\n",
        "\n",
        "Mastering JAX requires a bit of functional thinking, but there are libraries that allow a more pythonic/pytorchy model design. We will explore Haiku next."
      ],
      "metadata": {
        "id": "-d5ihf1tc-ig"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7ViyN6Q8kOa"
      },
      "source": [
        "## Haiku -- object-oriented neural network library on top of JAX\n",
        "\n",
        "[Haiku](https://github.com/deepmind/dm-haiku) is a simple neural network library for JAX that enables users to use familiar object-oriented programming models while allowing full access to JAX's pure function transformations.\n",
        "\n",
        "This colab goes through a complete but minimal example training an MLP classifier on MNIST. The [quickstart](https://github.com/deepmind/dm-haiku#quickstart) and [user-manual](https://github.com/deepmind/dm-haiku#user-manual) provide more information.\n",
        "\n",
        "Notable functions / entities\n",
        "* `hk.Module`'s are Python classes that hold references to their own parameters, and methods that apply functions on user inputs.\n",
        "* `hk.transform`: converts non-pure (objects) functions into pure functions; returns an object with a pair of pure functions `init` and `apply`.\n",
        "* `hk.next_rng_key()`: returns a unique random key\n",
        "\n",
        "**Important.**\n",
        "Do not use jax transforms (grad, jit, etc) with impure functions (e.g. inside Haiku networks). Instead, use them on `hk.transform`ed pure functions, or try the experimental `haiku.grad` and `haiku.jit` etc."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing Haiku"
      ],
      "metadata": {
        "id": "xO4YkwlaQ0gD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We will install and import haiku\n",
        "!pip install -q dm-haiku optax\n",
        "import haiku as hk"
      ],
      "metadata": {
        "id": "_U512EJVvQpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining Haiku modules"
      ],
      "metadata": {
        "id": "BZGn2q2nQ3nT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inherit from the hk.Module base class\n",
        "class MyLinear(hk.Module):\n",
        "\n",
        "  # Define module init, which usually takes in parameters\n",
        "  # like output_size that make the module configurable and reusable\n",
        "  def __init__(self, output_size, name=None):\n",
        "    super().__init__(name=name)\n",
        "    self.output_size = output_size\n",
        "\n",
        "  # Define a call function that will be the module's forward pass.\n",
        "  # Notice that parameters are not passed in here so the function is not pure.\n",
        "  def __call__(self, x):\n",
        "    j, k = x.shape[-1], self.output_size\n",
        "    # Use get_parameter to retrieve parameters\n",
        "    w = hk.get_parameter(\"w\", shape=[j, k], dtype=x.dtype, init=jnp.ones)\n",
        "    b = hk.get_parameter(\"b\", shape=[k], dtype=x.dtype, init=jnp.zeros)\n",
        "    return jnp.dot(x, w) + b"
      ],
      "metadata": {
        "id": "h6a51f9lQ6Jp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To use the module, we need to transform it to a pure function\n",
        "my_linear = hk.transform(lambda x: MyLinear(output_size=10)(x))"
      ],
      "metadata": {
        "id": "6KnZtTbnDc28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The transform returns the init() and apply() functions\n",
        "# init returns the initial parameters of the module\n",
        "rng = jax.random.PRNGKey(0)\n",
        "x = jnp.ones((2, 8))\n",
        "params = my_linear.init(rng, x)\n",
        "\n",
        "# We need to pass in a random key jax.random.PRNGKey(0)\n",
        "# to apply so that haiku can use it internally\n",
        "print(jax.tree_map(lambda x: x.shape, params))\n",
        "print(my_linear.apply(params, rng, x))"
      ],
      "metadata": {
        "id": "UF8FiONWDd5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build your own Haiku\n",
        "The `get_parameter()` function is certainly accessing params that are not passed as arguments, which results in impure functions.\n",
        "For more details on how Haiku retrieves parameters and converts impure functions to pure, you can \"[Build your own Haiku!](https://dm-haiku.readthedocs.io/en/latest/notebooks/build_your_own_haiku.html)\"."
      ],
      "metadata": {
        "id": "1jQ9UNF_T6uw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example: Training an MLP for MNIST image classification"
      ],
      "metadata": {
        "id": "96tdcCU2Q67U"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3h4MVQzkck_R"
      },
      "source": [
        "### Setting up and connecting to GPU\n",
        "\n",
        "We also use\n",
        "\n",
        "* [optax](https://github.com/deepmind/optax) a gradient processing and optimization library.\n",
        "* [tensorflow datasets](https://www.tensorflow.org/datasets) to load and pre-process data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NG3krUq1WK_A"
      },
      "source": [
        "# Imports related to types\n",
        "from typing import Any, Mapping, Iterator, Tuple\n",
        "import enum\n",
        "\n",
        "import optax\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "\n",
        "# Don't forget to select GPU runtime environment in Runtime -> Change runtime type\n",
        "devices = jax.devices()\n",
        "if not str(devices[0]).startswith('gpu'):\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(devices[0]))\n",
        "\n",
        "# Define our container types\n",
        "OptState = Any\n",
        "Batch = Mapping[str, np.ndarray]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6r0EfeAqIZkS"
      },
      "source": [
        "### TFDS datasets (MNIST)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "JAX does not support data loading or preprocessing, therefore we use tensorflow datasets (TFDS).\n",
        "\n",
        "TFDS comes with existing datasets (such as MNIST) that have defined splits. TFDS follows a builder-style api where we can transform the dataset by chaining transformations. Some important ones are:\n",
        "- `repeat` - cycles the data items indefinitely\n",
        "- `cache` - caches the dataset in memory (only use with small datasets, and call before repeat)\n",
        "- `shuffle(buffer_size)` - shuffles the data points in a window of size `buffer_size`\n",
        "- `batch(batch_size)` - batches the elements into batches of `batch_size`\n",
        "- `as_numpy_iterator()` - converts tensorflow tensors to numpy arrays that jax can work with\n",
        "- `map(fn)` - maps the function onto the elements (fn needs to be in tensorflow code)"
      ],
      "metadata": {
        "id": "FD8iKiwRvtiP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDP4M0aVWpdB"
      },
      "source": [
        "# We use TF datasets; JAX does not support data loading or preprocessing.\n",
        "NUM_CLASSES = 10  # MNIST has 10 classes, corresponding to the different digits.\n",
        "def load_dataset(\n",
        "    split: str,\n",
        "    *,\n",
        "    is_training: bool,\n",
        "    batch_size: int,\n",
        ") -> Iterator[Batch]:\n",
        "  \"\"\"Loads the dataset as a generator of batches.\"\"\"\n",
        "  ds = tfds.load('mnist:3.*.*', split=split).cache().repeat()\n",
        "  if is_training:\n",
        "    ds = ds.shuffle(10 * batch_size, seed=0)\n",
        "  ds = ds.batch(batch_size)\n",
        "  return ds.as_numpy_iterator()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploring the dataset"
      ],
      "metadata": {
        "id": "GHiamzbGzOvU"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2OYWEKKgVEJ"
      },
      "source": [
        "# Function to display images\n",
        "MAX_IMAGES = 10\n",
        "def gallery(images, labels, title='Input images'):\n",
        "  class_dict = [u'zero', u'one', u'two', u'three', u'four', u'five', u'six', u'seven', u'eight', u'nine']\n",
        "  num_frames, h, w, num_channels = images.shape\n",
        "  num_frames = min(num_frames, MAX_IMAGES)\n",
        "  ff, axes = plt.subplots(1, num_frames,\n",
        "                          figsize=(30, 30),\n",
        "                          subplot_kw={'xticks': [], 'yticks': []})\n",
        "  if images.min() < 0:\n",
        "    images = (images + 1.) / 2.\n",
        "  for i in range(0, num_frames):\n",
        "    if num_channels == 3:\n",
        "      axes[i].imshow(np.squeeze(images[i]))\n",
        "    else:\n",
        "      axes[i].imshow(np.squeeze(images[i]), cmap='gray')\n",
        "    axes[i].set_title(class_dict[labels[i]], fontsize=28)\n",
        "    plt.setp(axes[i].get_xticklabels(), visible=False)\n",
        "    plt.setp(axes[i].get_yticklabels(), visible=False)\n",
        "  ff.subplots_adjust(wspace=0.1)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTtne8cbZABG"
      },
      "source": [
        "# Display some training images with their labels.\n",
        "# First, create a dataset iterator for fetching batches.\n",
        "display_dataset_iter = iter(load_dataset('train', is_training=True, batch_size=10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Images have shape=(10, 28, 28, 1), batches of 10 images of 28x28 size with 1 color channel\n",
        "next(display_dataset_iter)['image'].shape"
      ],
      "metadata": {
        "id": "SJmhwRhbx3ZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Labels have shape=(10, ) as they are integers between [0, 9]\n",
        "next(display_dataset_iter)['label'].shape"
      ],
      "metadata": {
        "id": "OTODP7m2yYLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2VFyzjwZXbQ"
      },
      "source": [
        "# Then get a batch and display it.\n",
        "display_batch = next(display_dataset_iter)\n",
        "gallery(display_batch['image'], display_batch['label'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training hyperparams"
      ],
      "metadata": {
        "id": "3htdVcBhP2dD"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyL8nAD-OTn9"
      },
      "source": [
        "# There are 60k examples in the training\n",
        "# data. We consider training batches of 1000 examples.\n",
        "# We train for 10 epochs with constant learning rate\n",
        "# of 1e-3.\n",
        "# We use weight decay (L2 regularization), adding\n",
        "# the sum of the l2 norms of the weights to the\n",
        "# loss with a weight of 1e-4.\n",
        "TRAIN_BATCH_SIZE = 1000  #@param\n",
        "EVAL_BATCH_SIZE = 10000\n",
        "NUM_EPOCHS = 30  #@param\n",
        "lr = 1e-3 #@param\n",
        "WEIGHT_DECAY = 1e-4  #@param\n",
        "\n",
        "TRAIN_NUM_EXAMPLES = 60000  # Number of training examples in MNIST."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00m3RHu0BTir"
      },
      "source": [
        "### Exercise: define an MLP model in Haiku"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MyMLP(hk.Module):\n",
        "\n",
        "  def __init__(self, hidden_sizes, num_outputs, name=None):\n",
        "    super().__init__(name=name)\n",
        "    self._hidden_sizes = hidden_sizes\n",
        "    self._num_outputs = num_outputs\n",
        "\n",
        "  def __call__(self, batch: Batch):\n",
        "    # The input images are 28x28x1, so we first flatten them to apply linear (fully-connected) layers.\n",
        "    x = hk.Flatten()(batch) # B x 784\n",
        "\n",
        "    # We apply a sequence of linear layers.\n",
        "    for hidden_size in self._hidden_sizes:\n",
        "      # TODO: Apply a hk.Linear\n",
        "      x =  hk.Linear(hidden_size)(x)\n",
        "      # TODO: Introduce non-linearily (e.g. jax.nn.relu)\n",
        "      x = jax.nn.relu(x)\n",
        "\n",
        "    # TODO: Apply a final linear layer\n",
        "    x = hk.Linear(self._num_outputs)(x) # B x num_outputs\n",
        "    return x"
      ],
      "metadata": {
        "id": "0reALqi804FL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Forward fn for MLP including shaping images to right dimensions\n",
        "def mlp_fn(batch: Batch) -> jnp.ndarray:\n",
        "  \"\"\"Standard LeNet-300-100 MLP network.\"\"\"\n",
        "  # The images are in [0, 255], uint8; we need to convert to float and normalise\n",
        "  x = batch['image'].astype(jnp.float32) / 255.\n",
        "  # TODO: Use your MLP\n",
        "  mlp = MyMLP(hidden_sizes=(300, 100), num_outputs=10, name=\"LeNet\")\n",
        "  return mlp(x)"
      ],
      "metadata": {
        "id": "-t72ji7Dt8J_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise: Can you apply a transformer?"
      ],
      "metadata": {
        "id": "Gk5lbhWTtq2K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyTransformer(hk.Module):\n",
        "  \"\"\"Simple transformer module.\"\"\"\n",
        "\n",
        "  def __init__(self, num_heads, num_blocks, attn_size, dropout_rate, name=None):\n",
        "    super().__init__(name=name)\n",
        "    self._num_heads = num_heads        # Number of attention heads.\n",
        "    self._num_blocks = num_blocks      # Number of blocks (attention + dense)\n",
        "    self._attn_size = attn_size        # Size of the (key, query, value) vectors\n",
        "    self._dropout_rate = dropout_rate  # Probability with which to apply dropout.\n",
        "\n",
        "  def __call__(self, embeddings: Batch):\n",
        "    _, seq_len, model_size = embeddings.shape\n",
        "    h = embeddings\n",
        "    for _ in range(self._num_blocks):\n",
        "      # Attention\n",
        "      attn_block = hk.MultiHeadAttention(\n",
        "          num_heads=self._num_heads,\n",
        "          key_size=self._attn_size,\n",
        "          model_size=model_size,\n",
        "          w_init=hk.initializers.VarianceScaling(2 / self._num_blocks)\n",
        "      )\n",
        "      h_norm = hk.LayerNorm(axis=-1, create_scale=True, create_offset=True)(h)\n",
        "      h_attn = attn_block(h_norm, h_norm, h_norm)\n",
        "      h_attn = hk.dropout(hk.next_rng_key(), self._dropout_rate, h_attn)\n",
        "      h = h + h_attn # residual\n",
        "\n",
        "      # Then the dense block.\n",
        "      dense_block = hk.Sequential([\n",
        "          hk.Linear(2*model_size),\n",
        "          jax.nn.gelu,\n",
        "          hk.Linear(model_size),\n",
        "      ])\n",
        "      h_norm = hk.LayerNorm(axis=-1, create_scale=True, create_offset=True)(h)\n",
        "      h_dense = dense_block(h_norm)\n",
        "      h_dense = hk.dropout(hk.next_rng_key(), self._dropout_rate, h_dense)\n",
        "      h = h + h_dense\n",
        "\n",
        "    return hk.LayerNorm(axis=-1, create_scale=True, create_offset=True)(h)"
      ],
      "metadata": {
        "id": "o6Jz_Gr56O68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7j58v6LhWjrQ"
      },
      "source": [
        "def transformer_fn(batch: Batch) -> jnp.ndarray:\n",
        "  \"\"\"Standard LeNet-300-100 MLP network.\"\"\"\n",
        "  # The images are in [0, 255], uint8; we need to convert to float and normalise\n",
        "  x = batch['image'].astype(jnp.float32) / 255.\n",
        "\n",
        "  # TODO: Shape the batch into embeddings of shape\n",
        "  # BATCH_SIZE x NUM_TOKENS x EMBEDDING_SIZE\n",
        "  # IDEA: You could use your MLP\n",
        "  mlp_embed = MyMLP((100,), 512, name=\"Embedding\")\n",
        "  x = mlp_embed(x)\n",
        "  embeddings = x.reshape((x.shape[0], 32, -1)) # 32 tokens of size 16\n",
        "\n",
        "  transformer = MyTransformer(num_heads=2, num_blocks=2, attn_size=16, dropout_rate=0.3)\n",
        "  transformed = transformer(embeddings)\n",
        "\n",
        "  # TODO: Shape the result into required number of classes\n",
        "  mlp = MyMLP((100,), 10, name=\"LeNet\")\n",
        "  return mlp(transformed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model selection\n",
        "Select the MLP first, we will get to the transformer exercise later."
      ],
      "metadata": {
        "id": "hhR_YrTRuMTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = 'mlp_fn' #@param [\"mlp_fn\", \"transformer_fn\"] {type:\"string\"}\n"
      ],
      "metadata": {
        "id": "RQN5Fp59Dvae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CH5-IvPkMftZ"
      },
      "source": [
        "### Retrieve pure functions for our model (`init`, `apply`) using `hk.transform`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9YzGJ_4WuE5"
      },
      "source": [
        "if model == 'mlp_fn':\n",
        "  net_fn = mlp_fn\n",
        "elif model == 'transformer_fn':\n",
        "  net_fn = transformer_fn\n",
        "else:\n",
        "  raise ValueError('Model unavailable')\n",
        "\n",
        "# Since we don't store additional state statistics, e.g. needed in batch norm,\n",
        "# we use `hk.transform`. When we use batch_norm, we will use `hk.transform_with_state`.\n",
        "net = hk.transform(net_fn)\n",
        "# We will use the pure functions as net.init(...) and net.apply(...)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GA81esw-OoBK"
      },
      "source": [
        "### Exercise: define the optimisation objective (loss)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OA1FdFSwWwtb"
      },
      "source": [
        "# We use Adam optimizer here. Others are possible, e.g. sgd with momentum.\n",
        "opt = optax.adam(lr)\n",
        "\n",
        "# Training loss: cross-entropy plus weight decay regularization\n",
        "def loss(params: hk.Params, batch: Batch) -> jnp.ndarray:\n",
        "  \"\"\"Compute the loss of the network, including L2 for regularization.\"\"\"\n",
        "\n",
        "  # Get network predictions (B x NUM_CLASSES)\n",
        "  logits = net.apply(params, random.PRNGKey(0), batch)\n",
        "\n",
        "  # Generate one_hot labels from index classes\n",
        "  labels = jax.nn.one_hot(batch['label'], NUM_CLASSES)\n",
        "\n",
        "  # TODO: Compute mean softmax cross entropy over the batch\n",
        "  softmax_xent = -jnp.sum(labels * jax.nn.log_softmax(logits))\n",
        "  softmax_xent /= labels.shape[0]\n",
        "\n",
        "  # Compute the weight decay loss by penalising the norm of parameters\n",
        "  l2_loss = 0.5 * sum(jnp.sum(jnp.square(p)) for p in jax.tree_util.tree_leaves(params))\n",
        "\n",
        "  return softmax_xent + WEIGHT_DECAY * l2_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SGCtelDQfOv"
      },
      "source": [
        "### Define training step (parameters update)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsjUbkMWQlt-"
      },
      "source": [
        "@jax.jit\n",
        "def update(\n",
        "    params: hk.Params,\n",
        "    opt_state: OptState,\n",
        "    batch: Batch,\n",
        ") -> Tuple[hk.Params, OptState]:\n",
        "  \"\"\"Learning rule (stochastic gradient descent).\"\"\"\n",
        "  # Use jax transformation `grad` to compute gradients;\n",
        "  # it expects the prameters of the model and the input batch\n",
        "  loss_value, grads = jax.value_and_grad(loss)(params, batch)\n",
        "\n",
        "  # Compute parameters updates based on gradients and optimiser state\n",
        "  updates, opt_state = opt.update(grads, opt_state)\n",
        "\n",
        "  # Apply updates to parameters\n",
        "  new_params = optax.apply_updates(params, updates)\n",
        "  return new_params, opt_state, loss_value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RubrTm2_WOiP"
      },
      "source": [
        "### Initialise the model, the optimiser and dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2a8lZDaAmH3B"
      },
      "source": [
        "# Make datasets for train and test\n",
        "train_dataset = load_dataset('train', is_training=True, batch_size=TRAIN_BATCH_SIZE)\n",
        "train_eval_dataset = load_dataset('train', is_training=False, batch_size=EVAL_BATCH_SIZE)\n",
        "test_eval_dataset = load_dataset('test', is_training=False, batch_size=EVAL_BATCH_SIZE)\n",
        "\n",
        "# Set up dataset iterators.\n",
        "train_ds_iterator = iter(train_dataset)\n",
        "train_eval_ds_iterator = iter(train_eval_dataset)\n",
        "test_eval_ds_iterator = iter(test_eval_dataset)\n",
        "\n",
        "# Draw a data batch\n",
        "batch = next(train_ds_iterator)\n",
        "# Initialize model\n",
        "params = net.init(jax.random.PRNGKey(42), batch)\n",
        "#Initialize optimiser\n",
        "opt_state = opt.init(params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2yunmBjWyPQ"
      },
      "source": [
        "### Visualise parameter shapes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euhMKRF_W5_D"
      },
      "source": [
        "# Let's see how many parameters in our network and their shapes\n",
        "jax.tree_map(lambda x: x.shape, params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KA_CZhFHQDDN"
      },
      "source": [
        "### Evaluation metric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlEMA3RCXBU_"
      },
      "source": [
        "# Classification accuracy\n",
        "@jax.jit\n",
        "def accuracy(params: hk.Params, batch: Batch) -> jnp.ndarray:\n",
        "  # Get network predictions\n",
        "  predictions = net.apply(params, random.PRNGKey(0), batch)\n",
        "  print(predictions.shape)\n",
        "  # Return accuracy = how many predictions match the ground truth\n",
        "  return jnp.mean(jnp.argmax(predictions, axis=-1) == batch['label'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uq1OddpfX-6y"
      },
      "source": [
        "### Accuracy of the untrained model (should be ~10%)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHzblZx6X9jH"
      },
      "source": [
        "# Run accuracy on the test dataset\n",
        "test_accuracy = accuracy(params, next(iter(test_eval_dataset)))\n",
        "print('Test accuracy %f '% test_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjeMJxKYaeN-"
      },
      "source": [
        "### Run one training step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejfhiuoiaiZ6"
      },
      "source": [
        "# First, let's do one step and check if the updates lead to decrease in error\n",
        "params, opt_state, loss_before_train = update(params, opt_state, batch)\n",
        "print('Loss before train %f' % loss_before_train)\n",
        "new_loss = loss(params, next(train_ds_iterator))\n",
        "new_loss_same_batch = loss(params, batch)\n",
        "print('Loss after one step of training, same batch %f, different batch %f' % (new_loss_same_batch, new_loss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAR5joBwV5cT"
      },
      "source": [
        "# Train/eval loop.\n",
        "print(\"Training..\")\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  for step in range(TRAIN_NUM_EXAMPLES // TRAIN_BATCH_SIZE):\n",
        "    # Evaluate classification accuracy on train & test sets.\n",
        "    # evaluate train_accuracy on train set\n",
        "    train_accuracy = accuracy(params, next(train_eval_ds_iterator))\n",
        "    # evaluate test_accuracy on test set\n",
        "    test_accuracy = accuracy(params, next(test_eval_ds_iterator))\n",
        "\n",
        "    # Update step on the next batch of training examples.\n",
        "    next_batch = next(train_ds_iterator)\n",
        "    params, opt_state, loss_value = update(params, opt_state, next_batch)\n",
        "\n",
        "  print('Epoch %d Loss %f Train / Test accuracy: %f / %f' % (\n",
        "      epoch+1, loss_value, train_accuracy, test_accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNYLWtvmao4i"
      },
      "source": [
        "### Visualise network predictions after training; most of the predictions should be correct."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGb8B6n6au9L"
      },
      "source": [
        "# Get predictions for the same batch\n",
        "predictions = net.apply(params, random.PRNGKey(2), batch)\n",
        "pred_labels = jnp.argmax(predictions, axis=-1)\n",
        "gallery(batch['image'], pred_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise (open ended): Image classifier\n",
        "Train a classifier on the [cifar10 tfds dataset](https://www.tensorflow.org/datasets/catalog/cifar10). This dataset comes with 60000 (5:1 train:test split) 32x32 colour images in 10 classes. Feel free to change all the necessary code, or use the CNN in the next section."
      ],
      "metadata": {
        "id": "mDhSCwkyytMy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifFR1Iq9YChf"
      },
      "source": [
        "## Flax -- alternative library on top of JAX\n",
        "\n",
        "[Flax](https://github.com/google/flax) is another neural network library and ecosystem for JAX designed for flexibility.\n",
        "\n",
        "The link above provides a good quick intro, and the [documentation](https://flax.readthedocs.io/en/latest/index.html) has good examples, including an [annotated MNIST Example](https://colab.sandbox.google.com/github/google/flax/blob/master/docs/notebooks/annotated_mnist.ipynb). We reproduce a version of the MNIST example here, that is analogous to the Haiku section above.\n",
        "\n",
        "Flax comes with:\n",
        "\n",
        "* **Neural network API** (`flax.linen`): Dense, Conv, {Batch|Layer|Group} Norm, Attention, Pooling, {LSTM|GRU} Cell, Dropout\n",
        "* **Optimizers** (`flax.optim`): SGD, Momentum, Adam, LARS, Adagrad, LAMB, RMSprop\n",
        "* And much more, including utilities, worked examples and tuned, large scale examples.\n",
        "\n",
        "While flax includes optimizers, we can use `optax` to optimize the parameters in `flax.linen` networks. That is the approach we will take here.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "narnOeEHkEqh"
      },
      "source": [
        "`haiku` and `flax` both provide an OOP interface for neural nets; they both produce `init()` and `apply()` functions for their modules. Some differences you will notice are:\n",
        "\n",
        "1. Slightly different ways of declaring and working with modules.\n",
        "2. Different signature for `apply()`.\n",
        "3. The common `flax` pattern of passing around a full \"TrainState\", while `haiku` tends to keep `params`, `optimizer_state`, etc separate.\n",
        "\n",
        "Due to these differences we will rewrite much of the training and evaluation functions from before -- even though the changes are fairly minimal.\n",
        "\n",
        "To showcase additional features, we will use a simple convnet and SGD with momentum optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kDzToFNj9SB"
      },
      "source": [
        "!pip install -q flax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQwghhw5j-kB"
      },
      "source": [
        "from flax import linen as nn           # The Linen API\n",
        "from flax.training import train_state  # Useful dataclass to keep train state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PlPTAi9kqAQ"
      },
      "source": [
        "### Define network\n",
        "\n",
        "Create a convolutional neural network with the Linen API by subclassing\n",
        "[`Module`](https://flax.readthedocs.io/en/latest/flax.linen.html#core-module-abstraction).\n",
        "Because the architecture in this example is relatively simpleâ€”you're just\n",
        "stacking layersâ€”you can define the inlined submodules directly within the\n",
        "`__call__` method and wrap it with the\n",
        "[`@compact`](https://flax.readthedocs.io/en/latest/flax.linen.html#compact-methods)\n",
        "decorator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bvZuQi6kg8T"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "  \"\"\"A simple CNN model.\"\"\"\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    x = x.astype(jnp.float32) / 255\n",
        "    x = nn.Conv(features=32, kernel_size=(3, 3))(x)\n",
        "    x = nn.relu(x)\n",
        "    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
        "    x = nn.Conv(features=64, kernel_size=(3, 3))(x)\n",
        "    x = nn.relu(x)\n",
        "    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
        "    x = x.reshape((x.shape[0], -1))  # flatten\n",
        "    x = nn.Dense(features=256)(x)\n",
        "    x = nn.relu(x)\n",
        "    x = nn.Dense(features=NUM_CLASSES)(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJJqE7gKpmye"
      },
      "source": [
        "We do not need to explicitly retrieve pure functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNABQ8yolWFK"
      },
      "source": [
        "### Create train state (and optimizer)\n",
        "\n",
        "A common pattern in Flax is to create a single dataclass that represents the\n",
        "entire training state, including step number, parameters, and optimizer state.\n",
        "\n",
        "Also adding optimizer & model to this state has the advantage that we only need\n",
        "to pass around a single argument to functions like `train_step()` (see below).\n",
        "\n",
        "Because this is such a common pattern, Flax provides the class\n",
        "[flax.training.train_state.TrainState](https://flax.readthedocs.io/en/latest/flax.training.html#train-state)\n",
        "that serves most basic use cases. Usually one would subclass it to add more data\n",
        "to be tracked, but in this example we can use it without any modifications."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqpHZf4jlqvz"
      },
      "source": [
        "def create_train_state(rng, learning_rate, momentum):\n",
        "  \"\"\"Creates initial `TrainState`.\"\"\"\n",
        "  cnn = CNN()\n",
        "  params = cnn.init(rng, jnp.ones([1, 28, 28, 1]))['params']\n",
        "  # In the haiku example, we used Adam. Let's use SGD with momentum here.\n",
        "  tx = optax.sgd(learning_rate, momentum)\n",
        "  return train_state.TrainState.create(\n",
        "      apply_fn=cnn.apply, params=params, tx=tx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_62B7uilq2V"
      },
      "source": [
        "### Define the optimisation objective (loss)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUgmndZ2p8Qk"
      },
      "source": [
        "def flax_loss_fn(params, batch):\n",
        "  logits = CNN().apply({'params': params}, batch['image'])\n",
        "\n",
        "  labels = jax.nn.one_hot(batch['label'], NUM_CLASSES)\n",
        "\n",
        "  # Compute mean softmax cross entropy over the batch\n",
        "  softmax_xent = -jnp.sum(labels * jax.nn.log_softmax(logits))\n",
        "  softmax_xent /= labels.shape[0]\n",
        "\n",
        "  # Compute the weight decay loss by penalising the norm of parameters\n",
        "  l2_loss = 0.5 * sum(jnp.sum(jnp.square(p)) for p in jax.tree_util.tree_leaves(params))\n",
        "\n",
        "  return softmax_xent + WEIGHT_DECAY * l2_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQ1z4jIyp8Wl"
      },
      "source": [
        "### Evaluation Metric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajBJc-syra8p"
      },
      "source": [
        "# Classification accuracy\n",
        "@jax.jit\n",
        "def flax_accuracy(params, batch):\n",
        "  # Get network predictions\n",
        "  predictions = CNN().apply({'params': params}, batch['image'])\n",
        "  # Return accuracy = how many predictions match the ground truth\n",
        "  return jnp.mean(jnp.argmax(predictions, axis=-1) == batch['label'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsukw533rbDL"
      },
      "source": [
        "### Define training step (parameters update)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQ_zigsOr9lF"
      },
      "source": [
        "@jax.jit\n",
        "def flax_train_step(state, batch):\n",
        "  \"\"\"Train for a single step.\"\"\"\n",
        "  grads = jax.grad(flax_loss_fn)(state.params, batch)\n",
        "  state = state.apply_gradients(grads=grads)\n",
        "  return state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMJTu6qBsxDY"
      },
      "source": [
        "### Initialize the model and the optimiser (train state)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gb-zKQgCs9au"
      },
      "source": [
        "rng = jax.random.PRNGKey(0)\n",
        "rng, init_rng = jax.random.split(rng)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRHuZrFfs3hI"
      },
      "source": [
        "# We define a new (higher) learning rate for SGD and momentum\n",
        "learning_rate = 0.01\n",
        "momentum = 0.9"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VWuXzS4s5c5"
      },
      "source": [
        "state = create_train_state(init_rng, learning_rate, momentum)\n",
        "del init_rng  # Must not be used anymore."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAML32ZCRgsw"
      },
      "source": [
        "# Print parameter shapes\n",
        "jax.tree_map(lambda x: x.shape, state.params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2wmsbVXr3A7"
      },
      "source": [
        "### Accuracy of the untrained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zw50IuwQr3HK"
      },
      "source": [
        "# Run accuracy on the test dataset\n",
        "test_accuracy = flax_accuracy(state.params, next(iter(test_eval_dataset)))\n",
        "print('Test accuracy %f '% test_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwJh6L-4lrHD"
      },
      "source": [
        "### Run training steps in a loop. We also run evaluation periodically."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_9IBXoVwFZJ"
      },
      "source": [
        "# Train/eval loop.\n",
        "print(\"Getting data iterators..\")\n",
        "train_ds_iterator = iter(train_dataset)\n",
        "train_eval_ds_iterator = iter(train_eval_dataset)\n",
        "test_eval_ds_iterator = iter(test_eval_dataset)\n",
        "\n",
        "print(\"Training..\")\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  for step in range(TRAIN_NUM_EXAMPLES // TRAIN_BATCH_SIZE):\n",
        "    # Do SGD on a batch of training examples.\n",
        "    state = flax_train_step(state, next(train_ds_iterator))\n",
        "  # Periodically evaluate classification accuracy on train & test sets.\n",
        "  train_accuracy = flax_accuracy(state.params, next(train_eval_ds_iterator))\n",
        "  test_accuracy = flax_accuracy(state.params, next(test_eval_ds_iterator))\n",
        "  train_accuracy, test_accuracy = jax.device_get(\n",
        "      (train_accuracy, test_accuracy))\n",
        "  print('epoch %d Train / Test accuracy: %f / %f' % (epoch+1, train_accuracy, test_accuracy))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}